{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP64XWyvNkZl"
   },
   "source": [
    "Main Leaderboard\n",
    "\n",
    "Forecaster  | Peer score against Pros | % Beat Pros on same Qs (Bootstrap) | % Beat Pros on same Qs (Resample)\n",
    "\n",
    "Resample: Linear interpolation of two forecasters to determine \"ground truth\" for resampling. \"Ground truth\" = weighted combination of percent forecasts on each question that yields the most accurate score. Weights are 0 to 1.\n",
    "\n",
    "Correlation: Deal with by assigning weights to questions. Weights for independent questions are 1. \"Approximately correct rather than precisely wrong.\" Would be good to have a rule of thumb. These weights can be used for leaderboard above.\n",
    "\n",
    "Likely want % beat Pros to be beyond 95% for significance - but obviously a sliding scale.\n",
    "\n",
    "Can use this method for Metaculus Track Record! Have separate line for each platform - so can see significance against each platform. Then combine all competeing platforms and treat as the same forecaster going head-to-head against Metaculus!\n",
    "\n",
    "Heroku:\n",
    "\n",
    "Bots: https://data.heroku.com/dataclips/bmlboxtaewpwemfvqwktqxernfeq\n",
    "\n",
    "Pros: https://data.heroku.com/dataclips/rozqhydlvqrzsllgmioruallozjx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISzIoto4hnoG"
   },
   "outputs": [],
   "source": [
    "# @title Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize_scalar\n",
    "from datetime import datetime\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create df_bot_resolved_questions, df_pro_resolved_questions, df_pro_bot_resolved_questions, df_bot_question_weights\n",
    "\n",
    "\"\"\"\n",
    "Input question data for both bots and pros.\n",
    "\n",
    "Only look at questions that have resolved Yes or No.\n",
    "\n",
    "df_pro_resolved_questions: Has pro_question_id, title, resolution, scheduled_close_time\n",
    "df_bot_resolved_questions: Has bot_question_id, title, resolution, scheduled_close_time\n",
    "\n",
    "All pro questions are asked to bots, but not all bot questions are asked to pros (correction:\n",
    "not true in 2024 Q4, there were some that got launched to pros first? and were bad so they\n",
    "didn't get asked of bots?)\n",
    "\n",
    "To compare pros to bots, we need to match the pro_question_id with the bot_question_id.\n",
    "This is done by matching the title and scheduled_close_time.\n",
    "\n",
    "We remove early closers from the analysis. I do this by comparing actual close time to scheduled\n",
    "close time in a later cell!\n",
    "\n",
    "df_pro_bot_resolved_questions: Has pro_question_id, bot_question_id, title, resolution, scheduled_close_time, question_weight\n",
    "\"\"\"\n",
    "\n",
    "df_bot_scores = pd.read_csv(f'https://data.heroku.com/dataclips/nudnpycciffydoeihwbtttlkwpcj.csv')\n",
    "#df_bot_scores = pd.read_csv('scores/bots_score_data_q3.csv')\n",
    "df_bot_questions = df_bot_scores.rename(columns={'question_id': 'bot_question_id', 'question_title': 'title'})\n",
    "df_bot_questions = df_bot_questions[df_bot_questions['resolution'].isin(['yes', 'no'])]\n",
    "\n",
    "df_pro_scores = pd.read_csv(f'https://data.heroku.com/dataclips/dgoglqeavaxrhhnfcikkoobuollk.csv')\n",
    "#df_pro_scores = pd.read_csv('scores/pros_score_data_q3.csv')\n",
    "df_pro_questions = df_pro_scores.rename(columns={'question_id': 'pro_question_id', 'question_title': 'title'})\n",
    "df_pro_questions = df_pro_questions[df_pro_questions['resolution'].isin(['yes', 'no'])]\n",
    "\n",
    "df_pro_resolved_questions = df_pro_questions[['pro_question_id', 'title', 'resolution', 'scheduled_close_time', 'actual_close_time', 'question_weight']]\n",
    "df_bot_resolved_questions = df_bot_questions[['bot_question_id', 'title', 'resolution', 'scheduled_close_time', 'actual_close_time', 'question_weight']]\n",
    "\n",
    "df_pro_bot_resolved_questions = pd.merge(\n",
    "    df_bot_resolved_questions,\n",
    "    df_pro_resolved_questions[['pro_question_id', 'title', 'scheduled_close_time', 'question_weight']],\n",
    "    on=['title', 'scheduled_close_time'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_resolved_questions['question_weight'] = df_pro_bot_resolved_questions['question_weight_x'].combine_first(df_pro_bot_resolved_questions['question_weight_y'])\n",
    "df_pro_bot_resolved_questions.drop(['question_weight_x', 'question_weight_y'], axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df_pro_bot_resolved_questions = df_pro_bot_resolved_questions.drop_duplicates()\n",
    "\n",
    "# Cast both question ids to int64\n",
    "df_pro_bot_resolved_questions['pro_question_id'] = df_pro_bot_resolved_questions['pro_question_id'].astype('Int64')\n",
    "df_pro_bot_resolved_questions['bot_question_id'] = df_pro_bot_resolved_questions['bot_question_id'].astype('Int64')\n",
    "\n",
    "# Remove df_bot_resolved_questions and df_pro_resolved_questions to make sure you only ever use df_pro_bot_resolved_questions\n",
    "del df_bot_resolved_questions\n",
    "del df_pro_resolved_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TK: See whether Cassie's intervals on peer score is in the same range as Q3 - if a lot of people \"burst their bounds\" that would be kind of weird (tom)\n",
    "# If no one updated their bots from Q3 to Q4, then 95% of the bots should land within their bounds\n",
    "# ... But people did update their bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are any of the weights NOT 1 -- for Q3 we need to assign weights \"manually,\" oops\n",
    "print(df_pro_bot_resolved_questions[df_pro_bot_resolved_questions['question_weight'] != 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Relationships between Bot Questions, create df_bot_question_related_weights (FOR Q3 ONLY)\n",
    "if 25871 in df_pro_bot_resolved_questions['bot_question_id'].values:\n",
    "  \"\"\"\n",
    "  Relationships between questions are entered as tuples. These relationships\n",
    "  will be used to perform logical consistency checks.\n",
    "\n",
    "  Weights are assigned to questions based on relationships. This is a way to\n",
    "  deal with correlations between questions.\n",
    "  \"\"\"\n",
    "\n",
    "  # Scope sensitity list of tuples where the first entry should equal the sum of the others\n",
    "  bot_scope_questions = [\n",
    "        (26019, 26017, 26018), # Starship launches\n",
    "        (26098, 26096, 26097), # SENSEX\n",
    "        (26159, 26158, 26157), # Geomagnetic storm July 28\n",
    "        (26194, 26195, 26196), # measles cases\n",
    "        (26006, 26005, 26004), # Trump lead over Biden\n",
    "        (26642, 26643, 26644), # spanish wikipedia\n",
    "        (26700, 26701, 26702), # market cap cryptocurrencies\n",
    "        (27261, 27262, 27263), # Geomagnetic storm Sept 11\n",
    "        ]\n",
    "\n",
    "  # Sum of each tuple should logically equal 1\n",
    "  bot_sum_to_1_questions = [\n",
    "      (25952, 25953, 25954), # French PM party July 30\n",
    "      (25957, 25958, 25959), # Tour de France winner\n",
    "      (26570, 26571, 26572, 26573), # Warhammer\n",
    "      (26574, 26575, 26576, 26577), # H5 cases in US\n",
    "      (26671, 26670, 26669), # DOES NOT SUM TO EXACTLY 1 PM France Aug 31\n",
    "      (27748, 27747, 27746, 27749), # Speed Chess\n",
    "      (27488, 27489, 27490, 27491, 27492, 27493), # August CPI\n",
    "      (27932, 27933, 27934, 27935), # Chinese youth unemployment\n",
    "      (27484, 27485, 27486, 27487), # Fed rate cut Sept meeting\n",
    "      (28045, 28044, 28043, 28042), # Afd vote share\n",
    "      (28038, 28039, 28040, 28041), # Major Atlantic hurricanes\n",
    "      (26776, 26777, 26778, 26779), # Seattle-Tacoma-Bellevu Air Quality\n",
    "      ]\n",
    "\n",
    "  # parent, child, if_yes, if_no\n",
    "  bot_conditional_pair = [\n",
    "      (26917, 26918, 26919, 26920) # israel lebanon conflict\n",
    "  ]\n",
    "\n",
    "  # CDFs - Logically the probability of each successive question must not decrease\n",
    "  bot_increasing_questions = [\n",
    "      (26981, 26982, 26983, 26984, 26985, 26986), # aircraft ADIZ\n",
    "      (26977, 26978, 26979, 26980), # hurricane energy\n",
    "      (27548, 27547, 27546, 27545), # mpox CDC risk level\n",
    "      (28306, 28305, 28304, 28303, 28302), # Gas prices in US Sept 30\n",
    "  ]\n",
    "\n",
    "  bot_repeated_questions = [\n",
    "      (26646, 26021), # mens 100m dash record\n",
    "      (26555, 27021), # USA gold silver\n",
    "      (26210, 26917), # israel invade lebanon\n",
    "      (26781, 26304), # ruto\n",
    "      (26100, 27136), # rfk drop out\n",
    "      (25956, 27158), # democrat brokered convention\n",
    "      (26102, 27022), # astronauts NOT EXACT REPEAT\n",
    "      (26022, 27085), # arrest warrants NOT EXACT REPEAT\n",
    "      (26235, 27281), # Buffett Indicator\n",
    "      (26390, 27789), # Bubble Magnificent 7\n",
    "      (26024, 27161), # QB Bo Nix starting for Broncos\n",
    "      (26302, 27282), # riots\n",
    "      (25955, 27157), # armed forces death US, China, Japan\n",
    "      (26958, 27640), # Youtube banned in Russia\n",
    "      (25936, 27141), # Crimean bridge attack\n",
    "  ]\n",
    "\n",
    "  bot_similar_questions = [\n",
    "      (26915, 26916), # harris favorability\n",
    "      (26913, 26914), # trump favorability\n",
    "      (26193, 27733), # debate on Sept 10\n",
    "      (27886, 27968), # Taylor Swift awards\n",
    "      (27723, 27637), # Best Rock VMAs\n",
    "      (27583, 27582, 27584, 27602, 27603, 27604), # mpox Zambia, US, Angola, Russia, Japan, Mexico\n",
    "      (26306, 26838), # Richest people 250th > $10.2, 500th > 6.2\n",
    "      (27887, 27969), # Emmys Outstanding Limited or Anthology Series\n",
    "      (28206, 28207, 28208, 28209, 28210), # LMSYS leaderboard\n",
    "      (28154, 28336), # Nigeria Edo gubernatorial election\n",
    "      (26407, 27897), # Second Russian mobilization wave\n",
    "      (27539, 26215), # Nuclear weapons used\n",
    "      (27606, 27607, 27608, 27609, 27610), # Ukranian forces capture\n",
    "      (26387, 27788), # Will Tesla increase deliveries in Q3 2024\n",
    "      (26821, 26959), # VP debate\n",
    "      (26212, 26213, 26214), # number of dairy cow herds with H5N1\n",
    "      (26639, 26640, 26641), # Presidential debate 0, 1, or 2+\n",
    "  ]\n",
    "\n",
    "  ####### CREATE QUESTION WEIGHTS #########\n",
    "\n",
    "  # Combine both lists of tuples\n",
    "  all_questions = bot_scope_questions + bot_sum_to_1_questions + bot_increasing_questions + bot_similar_questions + bot_conditional_pair\n",
    "\n",
    "  # Create an empty list to store the data\n",
    "  data = []\n",
    "\n",
    "  # Process each tuple\n",
    "  for tuple_questions in all_questions:\n",
    "      # Calculate the weight for each question in the tuple\n",
    "      weight = np.log2(1 + len(tuple_questions))/(1 + len(tuple_questions))\n",
    "\n",
    "      # Add each question and its weight to the data list\n",
    "      for question_id in tuple_questions:\n",
    "          data.append({'bot_question_id': question_id, 'question_weight': weight})\n",
    "\n",
    "  # Process each tuple\n",
    "  for tuple_questions in bot_repeated_questions:\n",
    "      # 1st iteration has weight 1, 2nd has weight 1/2, 3rd weight 1/3....\n",
    "      count = 1\n",
    "\n",
    "      # Add each question and its weight to the data list\n",
    "      for question_id in tuple_questions:\n",
    "          data.append({'bot_question_id': question_id, 'question_weight': 1/count})\n",
    "          count += 1\n",
    "\n",
    "  # Create the DataFrame\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "  # Sort the DataFrame by bot_question_id for better readability\n",
    "  df_bot_question_related_weights = df.sort_values('bot_question_id').reset_index(drop=True)\n",
    "\n",
    "# if df_bot_question_related_weights is defined, replace the question weights in df_pro_bot_resolved_questions\n",
    "if 'df_bot_question_related_weights' in locals():\n",
    "    df_pro_bot_resolved_questions = pd.merge(\n",
    "        df_pro_bot_resolved_questions,\n",
    "        df_bot_question_related_weights,\n",
    "        on='bot_question_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    df_pro_bot_resolved_questions['question_weight'] = df_pro_bot_resolved_questions['question_weight_y'].combine_first(df_pro_bot_resolved_questions['question_weight_x'])\n",
    "    df_pro_bot_resolved_questions.drop(['question_weight_x', 'question_weight_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any non-1 weights (there should be)\n",
    "print(df_pro_bot_resolved_questions[df_pro_bot_resolved_questions['question_weight'] != 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique pro questions, bot questions\n",
    "pro_questions = df_pro_bot_resolved_questions['pro_question_id'].unique()\n",
    "bot_questions = df_pro_bot_resolved_questions['bot_question_id'].unique()\n",
    "print(pro_questions, bot_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove early closers IF right now is before scheduled close time\n",
    "df_pro_bot_resolved_questions['scheduled_close_time'] = pd.to_datetime(df_pro_bot_resolved_questions['scheduled_close_time']).dt.tz_localize(None)\n",
    "df_pro_bot_resolved_questions['actual_close_time'] = pd.to_datetime(df_pro_bot_resolved_questions['actual_close_time']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_early_closers = False # SET TO FALSE WHEN ALL Q'S ARE RESOLVED\n",
    "if remove_early_closers:\n",
    "  df_pro_bot_resolved_questions = df_pro_bot_resolved_questions[(df_pro_bot_resolved_questions['actual_close_time'] <= df_pro_bot_resolved_questions['scheduled_close_time'])]\n",
    "\n",
    "print('Number of unique questions in df_pro_bot_resolved_questions:', len(df_pro_bot_resolved_questions['bot_question_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read in the scores dataclips from heroku, take last (spot) score for each question_id, forecaster pair; make it into what Tom's code expects\n",
    "\n",
    "## BOTS\n",
    "\n",
    "# BASELINE\n",
    "df_bot_baseline = df_bot_scores[df_bot_scores['score_type'] == 'spot_baseline']\n",
    "\n",
    "# Take the LAST score for each (forecaster, question_id) pair\n",
    "df_bot_baseline = df_bot_baseline.groupby(['question_id', 'forecaster']).last().reset_index()\n",
    "\n",
    "# PEER\n",
    "df_bot_peer = df_bot_scores[df_bot_scores['score_type'] == 'spot_peer']\n",
    "\n",
    "# Take the LAST score for each (forecaster, question_id) pair\n",
    "df_bot_peer = df_bot_peer.groupby(['question_id', 'forecaster']).last().reset_index()\n",
    "\n",
    "## PROS\n",
    "\n",
    "# BASELINE\n",
    "df_pro_baseline = df_pro_scores[df_pro_scores['score_type'] == 'spot_baseline']\n",
    "\n",
    "# Take the LAST score for each (forecaster, question_id) pair\n",
    "df_pro_baseline = df_pro_baseline.groupby(['question_id', 'forecaster']).last().reset_index()\n",
    "df_pro_baseline_long = df_pro_baseline.copy()\n",
    "\n",
    "# ADD THE BOT MEDIAN SPOT SCORE & REMOVE UNNECESSARY COLUMNS\n",
    "df_bot_baseline = df_bot_baseline[['question_id', 'question_title', 'question_weight', 'forecaster', 'score', 'resolution']]\n",
    "\n",
    "# Calculate medians and preserve metadata\n",
    "medians = (df_bot_baseline\n",
    "    .groupby(['question_id', 'question_title', 'question_weight'])['score']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .assign(forecaster='bot_median')\n",
    ")\n",
    "\n",
    "# Combine with original data\n",
    "df_bot_baseline = pd.concat([df_bot_baseline, medians])\n",
    "df_bot_baseline_long = df_bot_baseline.copy()\n",
    "\n",
    "# DO THE SAME FOR DF_BOT_PEER\n",
    "df_bot_peer = df_bot_peer[['question_id', 'question_title', 'question_weight', 'forecaster', 'score', 'resolution']]\n",
    "\n",
    "# Get all columns except score and forecaster (TK: Does it make sense to take median of peer scores?)\n",
    "medians = (df_bot_peer\n",
    "    .groupby(['question_id', 'question_title', 'question_weight'])['score']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .assign(forecaster='bot_median')\n",
    ")\n",
    "\n",
    "# Combine with original data\n",
    "df_bot_peer = pd.concat([df_bot_peer, medians])\n",
    "\n",
    "df_bot_baseline_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: FILTER TO ONLY THOSE QUESTION ID's THAT ARE IN DF_PRO_BOT_RESOLVED_QUESTIONS\n",
    "df_bot_peer = df_bot_peer[df_bot_peer['question_id'].isin(df_pro_bot_resolved_questions['bot_question_id'])]\n",
    "df_bot_baseline = df_bot_baseline[df_bot_baseline['question_id'].isin(df_pro_bot_resolved_questions['bot_question_id'])]\n",
    "df_bot_baseline_long = df_bot_baseline_long[df_bot_baseline_long['question_id'].isin(df_pro_bot_resolved_questions['bot_question_id'])]\n",
    "df_pro_baseline = df_pro_baseline[df_pro_baseline['question_id'].isin(df_pro_bot_resolved_questions['pro_question_id'])]\n",
    "df_pro_baseline_long = df_pro_baseline_long[df_pro_baseline_long['question_id'].isin(df_pro_bot_resolved_questions['pro_question_id'])]\n",
    "df_bot_scores = df_bot_scores[df_bot_scores['question_id'].isin(df_pro_bot_resolved_questions['bot_question_id'])]\n",
    "df_pro_scores = df_pro_scores[df_pro_scores['question_id'].isin(df_pro_bot_resolved_questions['pro_question_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bots = df_bot_peer['forecaster'].unique()\n",
    "all_bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique score_type in df_bot_scores\n",
    "df_bot_scores['score_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_stats(df):\n",
    "    results = []\n",
    "    \n",
    "    # For each forecaster\n",
    "    for forecaster in df['forecaster'].unique():\n",
    "        forecaster_data = df[df['forecaster'] == forecaster]\n",
    "        \n",
    "        # Get scores and weights\n",
    "        scores = forecaster_data['score']\n",
    "        weights = forecaster_data['question_weight']\n",
    "        \n",
    "        # Calculate weighted mean\n",
    "        weighted_mean = np.average(scores, weights=weights)\n",
    "        weighted_sum = np.sum(scores * weights)\n",
    "        \n",
    "        # Calculate weighted standard error\n",
    "        # Using weighted variance formula\n",
    "        weighted_var = np.average((scores - weighted_mean)**2, weights=weights)\n",
    "        n = len(scores)\n",
    "        weighted_se = np.sqrt(weighted_var / n)\n",
    "        \n",
    "        # Calculate t-statistic for 95% confidence interval\n",
    "        t_value = stats.t.ppf(0.975, n-1)\n",
    "        ci_lower = weighted_mean - (t_value * weighted_se)\n",
    "        \n",
    "        results.append({\n",
    "            'forecaster': forecaster,\n",
    "            'weighted_mean': weighted_mean,\n",
    "            'weighted_sum': weighted_sum,\n",
    "            'n_questions': n,\n",
    "            'ci_lower': ci_lower,\n",
    "            'weighted_se': weighted_se\n",
    "        })\n",
    "    \n",
    "    # Convert to dataframe and sort by lower bound\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values('weighted_sum', ascending=False)\n",
    "\n",
    "# Calculate and show results\n",
    "ranked_forecasters = calculate_weighted_stats(df_bot_peer)\n",
    "\n",
    "ranked_forecasters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmAFBHIhK77X"
   },
   "outputs": [],
   "source": [
    "# @title Create df_bot_baseline, df_bot_peer, df_bot_forecasts, bots\n",
    "\n",
    "\"\"\"\n",
    "df_bot_baseline: Spot Baseline scores for all bots & bot_median\n",
    "\n",
    "df_bot_peer: Spot Peer scores for all bots & bot_median. Can be used to recreate\n",
    "the tournament leaderboard on the site.\n",
    "\n",
    "df_bot_forecasts: Spot forecasts for all bots & bot_median, ie only counts the\n",
    "final forecast\n",
    "\n",
    "bots: a list of all bots\n",
    "\"\"\"\n",
    "\n",
    "# Pivot df_bot_baseline\n",
    "df_bot_baseline = df_bot_baseline.rename(columns={'question_id': 'bot_question_id'})\n",
    "#df_bot_baseline['score'] = pd.to_numeric(df_bot_baseline['score'], errors='coerce')\n",
    "df_pivoted = df_bot_baseline.pivot(index='bot_question_id', columns='forecaster', values='score')\n",
    "df_pivoted = df_pivoted.reset_index()\n",
    "df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n",
    "\n",
    "# Move 'question_id' to be the first column\n",
    "cols = df_pivoted.columns.tolist()\n",
    "cols = ['bot_question_id'] + [col for col in cols if col != 'bot_question_id']\n",
    "df_pivoted = df_pivoted[cols]\n",
    "\n",
    "all_columns = df_pivoted.columns.tolist()\n",
    "# Remove 'question_id' and 'bot_median' from the list if they exist\n",
    "all_columns = [col for col in all_columns if col not in ['bot_question_id', 'bot_median']]\n",
    "new_column_order = ['bot_question_id', 'bot_median'] + all_columns\n",
    "df_pivoted = df_pivoted[new_column_order]\n",
    "df_bot_baseline_wide = df_pivoted\n",
    "df_bot_baseline_wide['bot_question_id'] = pd.to_numeric(df_bot_baseline_wide['bot_question_id'], errors='coerce')\n",
    "\n",
    "# Create df_bot_peer\n",
    "df_bot_peer = df_bot_peer.rename(columns={'question_id': 'bot_question_id'})\n",
    "df_bot_peer['score'] = pd.to_numeric(df_bot_peer['score'], errors='coerce')\n",
    "\n",
    "\n",
    "def make_wide(df_bot_peer):\n",
    "  df_pivoted = df_bot_peer.pivot(index='bot_question_id', columns='forecaster', values='score')\n",
    "  df_pivoted = df_pivoted.reset_index()\n",
    "  df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n",
    "\n",
    "  # Step 4: Move 'question_id' to be the first column\n",
    "  cols = df_pivoted.columns.tolist()\n",
    "  cols = ['bot_question_id'] + [col for col in cols if col != 'bot_question_id']\n",
    "  df_pivoted = df_pivoted[cols]\n",
    "\n",
    "  all_columns = df_pivoted.columns.tolist()\n",
    "  ## Remove 'question_id' and 'bot_median' from the list if they exist\n",
    "  all_columns = [col for col in all_columns if col not in ['bot_question_id']]\n",
    "  new_column_order = ['bot_question_id'] + all_columns\n",
    "  df_pivoted = df_pivoted[new_column_order]\n",
    "  df_bot_peer_wide = df_pivoted\n",
    "  df_bot_peer_wide['bot_question_id'] = pd.to_numeric(df_bot_peer_wide['bot_question_id'], errors='coerce')\n",
    "  \n",
    "  # Join with df_pro_bot_resolved_questions to get question weights\n",
    "  df_bot_peer_wide = pd.merge(\n",
    "      df_bot_peer_wide,\n",
    "      df_pro_bot_resolved_questions[['bot_question_id', 'question_weight']],\n",
    "      on='bot_question_id',\n",
    "      how='left'\n",
    "  )\n",
    "\n",
    "  return df_bot_peer_wide\n",
    "\n",
    "df_bot_peer_wide = make_wide(df_bot_peer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ CONVERT BASELINE SCORE TO FORECAST ###################\n",
    "\n",
    "def convert_baseline_to_forecasts(df):\n",
    "    # Assumes columns: 'bot_question_id' 'resolution'\n",
    "    result_df = df.copy()\n",
    "\n",
    "    def score_to_forecast(score, resolution):\n",
    "        if resolution == 'yes':\n",
    "            return 2 ** (score/100 - 1)\n",
    "        else:\n",
    "            return 1 - 2 ** (score/100 - 1)\n",
    "\n",
    "    score_columns = ['score']\n",
    "    #score_columns = [col for col in score_columns if col not in ['bot_question_id', 'resolution', 'pro_question_id']]\n",
    "\n",
    "    for col in score_columns:\n",
    "        result_df['forecast'] = result_df.apply(lambda row: score_to_forecast(row[col], row['resolution']), axis=1)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "df_bot_forecasts = convert_baseline_to_forecasts(df_bot_baseline)\n",
    "df_bot_forecasts = df_bot_forecasts.drop('resolution', axis=1)\n",
    "\n",
    "df_bot_forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XceLWcgCPNw-"
   },
   "outputs": [],
   "source": [
    "# @title Bot Baseline Leaderboard\n",
    "\n",
    "# Calculate the total score for each bot\n",
    "total_scores = df_bot_baseline_wide.iloc[:, 1:].fillna(0).sum()\n",
    "\n",
    "# Create a new dataframe with the total scores\n",
    "df_total_scores = pd.DataFrame({'Bot': total_scores.index, 'Baseline_Score': total_scores.values})\n",
    "\n",
    "# Sort the dataframe by Total_Score in descending order\n",
    "df_total_scores_sorted = df_total_scores.sort_values('Baseline_Score', ascending=False)\n",
    "\n",
    "# Add a Rank column\n",
    "df_total_scores_sorted['Rank'] = range(1, len(df_total_scores_sorted) + 1)\n",
    "\n",
    "# Set Rank as the index\n",
    "df_total_scores_ranked = df_total_scores_sorted.set_index('Rank')\n",
    "\n",
    "# Display the result\n",
    "df_total_scores_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iRDMoH7hTBEq"
   },
   "outputs": [],
   "source": [
    "# @title Bot Peer Leaderboard\n",
    "\n",
    "\"\"\"\n",
    "NOTE: This can be different from the leaderboad on the site IF early closers\n",
    "are excluded (check remove_early_closers bool).\n",
    "\"\"\"\n",
    "\n",
    "df_filled = df_bot_peer_wide.fillna(0)\n",
    "#df_filled = df_filled.drop(['bot_question_id', 'question_weight'], axis=1)\n",
    "\n",
    "# Calculate the total score for each player\n",
    "total_scores = df_filled.sum()\n",
    "\n",
    "# Create a new DataFrame for the leaderboard\n",
    "leaderboard = pd.DataFrame({\n",
    "    'bot': total_scores.index,\n",
    "    'Peer Score': total_scores.values\n",
    "})\n",
    "\n",
    "# Remove bot_question_id from the leaderboard\n",
    "leaderboard = leaderboard[leaderboard['bot'] != 'bot_question_id']\n",
    "\n",
    "# Sort the leaderboard by Total Score in descending order\n",
    "leaderboard = leaderboard.sort_values('Peer Score', ascending=False)\n",
    "\n",
    "# Reset the index and add a 'Rank' column\n",
    "leaderboard = leaderboard.reset_index(drop=True)\n",
    "leaderboard.index += 1\n",
    "leaderboard.index.name = 'Rank'\n",
    "\n",
    "# Display the leaderboard\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD PRO_MEDIAN ROWS\n",
    "\n",
    "df_pro_scores = df_pro_baseline\n",
    "\n",
    "# Create pro_median: for each question, calculate the median score\n",
    "median_scores = df_pro_scores.groupby('question_id')['score'].median().reset_index()\n",
    "median_scores = median_scores.rename(columns={'score': 'median_score'})  # rename to avoid confusion\n",
    "\n",
    "# Create template rows with metadata\n",
    "template_rows = df_pro_scores.groupby('question_id').first().reset_index()\n",
    "\n",
    "# Create new rows with median scores\n",
    "median_rows = template_rows.merge(median_scores, on='question_id', how='left')\n",
    "median_rows['forecaster'] = 'pro_median'\n",
    "median_rows['score'] = median_rows['median_score']  # assign the median to score column\n",
    "median_rows = median_rows.drop('median_score', axis=1)  # clean up temporary column\n",
    "\n",
    "# Concatenate original DataFrame with new median rows\n",
    "df_combined = pd.concat([df_pro_scores, median_rows], ignore_index=True)\n",
    "\n",
    "df_pro_scores = df_combined\n",
    "\n",
    "df_combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bot_scores = df_bot_scores[df_bot_scores['score_type'] == 'spot_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "j6txsgls_QrC"
   },
   "outputs": [],
   "source": [
    "# @title Create df_pro_baseline, df_pro_forecasts\n",
    "\n",
    "\"\"\"\n",
    "WARNING: This may not exclude early closers.\n",
    "\n",
    "df_pro_baseline: Spot Baseline scoes for all pros & pro_median\n",
    "\n",
    "df_pro_forecasts: Spot forecasts for all pros & pro_median\n",
    "\"\"\"\n",
    "\n",
    "# Pivot df_pro_scores (baseline)\n",
    "#df_pro_scores = df_pro_scores.rename(columns={'question_id': 'pro_question_id'})\n",
    "#df_pivoted = df_pro_scores.pivot(index='pro_question_id', columns='forecaster', values='score')\n",
    "#df_pivoted = df_pivoted.reset_index()\n",
    "#df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n",
    "\n",
    "#cols = df_pivoted.columns.tolist()\n",
    "#cols = ['pro_question_id'] + [col for col in cols if col != 'pro_question_id']\n",
    "#df_pivoted = df_pivoted[cols]\n",
    "\n",
    "#all_columns = df_pivoted.columns.tolist()\n",
    "#all_columns = [col for col in all_columns if col not in ['pro_question_id', 'pro_median']]\n",
    "#new_column_order = ['pro_question_id', 'pro_median'] + all_columns\n",
    "#df_pivoted = df_pivoted[new_column_order]\n",
    "#df_pro_baseline = df_pivoted\n",
    "#df_pro_baseline['pro_question_id'] = pd.to_numeric(df_pro_baseline['pro_question_id'], errors='coerce')\n",
    "\n",
    "\n",
    "#print(f\"Shape of df_pro_baseline: {df_pro_baseline.shape}\")\n",
    "#print(df_pro_baseline.columns)\n",
    "\n",
    "\n",
    "#df_temp = pd.merge(\n",
    "    #df_pro_baseline,\n",
    "    #df_pro_scores[['pro_question_id', 'resolution']],\n",
    "    #on='pro_question_id',\n",
    "    #how='left'\n",
    "#)\n",
    "\n",
    "df_pro_forecasts = convert_baseline_to_forecasts(df_pro_scores)\n",
    "df_pro_forecasts = df_pro_forecasts.drop('resolution', axis=1)\n",
    "\n",
    "df_pro_forecasts.head()\n",
    "\n",
    "#print(f\"Shape of df_pro_forecasts: {df_pro_forecasts.shape}\")\n",
    "#print(df_pro_forecasts.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Yfq0_lDKAMl7"
   },
   "outputs": [],
   "source": [
    "# @title Create df_pro_bot_forecasts, df_bot_vs_pro_peer, df_bot_vs_pro_leaderboard, df_bot_vs_pro_weighted_leaderboard\n",
    "\n",
    "\"\"\"\n",
    "df_pro_bot_forecasts: Spot forecasts for all bots & pro_median, question resolutions, and question weights\n",
    "\n",
    "df_bot_vs_pro_peer: Calculates Peer scores as if there is a tournament with only\n",
    "a single bot and the pro_median. This is the main comparison metric for\n",
    "assessing how a bot compares to the human aggregate. Positive scores mean that\n",
    "the bot did better than the pro_median. Negative scores mean that the bot did\n",
    "worse than the pro_median.\n",
    "\n",
    "df_bot_vs_pro_leaderboard: A leaderboard based on df_bot_vs_pro_peer.\n",
    "\n",
    "df_bot_vs_pro_weighted_leaderboard: A leaderboard based on df_bot_vs_pro_peer\n",
    "with question weighting.\n",
    "\"\"\"\n",
    "\n",
    "# Now pivot df_pro_forecasts; forecaster = columns; forecast = values; index = pro_question_id\n",
    "df_pro_forecasts = df_pro_forecasts.rename(columns={'question_id': 'pro_question_id'})\n",
    "df_pro_forecasts = df_pro_forecasts.pivot(index='pro_question_id', columns='forecaster', values='forecast')\n",
    "# Make the index a column and make it numeric\n",
    "df_pro_forecasts = df_pro_forecasts.reset_index()\n",
    "\n",
    "# Now pivot df_bot_forecasts; forecaster = columns; forecast = values; index = pro_question_id\n",
    "df_bot_forecasts = df_bot_forecasts.rename(columns={'question_id': 'bot_question_id'})\n",
    "df_bot_forecasts = df_bot_forecasts.pivot(index='bot_question_id', columns='forecaster', values='forecast')\n",
    "# Make the index a column and make it numeric\n",
    "df_bot_forecasts = df_bot_forecasts.reset_index()\n",
    "\n",
    "# One row per question, with pro_question_id and bot_question_id and resolution\n",
    "df_pro_bot_resolved_questions_first = df_pro_bot_resolved_questions.groupby(['pro_question_id', 'bot_question_id']).first().reset_index()[['pro_question_id', 'bot_question_id', 'resolution', 'question_weight']]\n",
    "\n",
    "df2 = pd.merge(\n",
    "    df_pro_bot_resolved_questions_first,\n",
    "    df_pro_forecasts[['pro_question_id', 'pro_median']],\n",
    "    on='pro_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_forecasts = pd.merge(\n",
    "    df2,\n",
    "    df_bot_forecasts,\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_forecasts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Shape of df_pro_bot_forecasts: {df_pro_bot_forecasts.shape}\")\n",
    "#print(df_pro_bot_forecasts.columns)\n",
    "\n",
    "# Create a new DataFrame to store peer scores\n",
    "df_bot_vs_pro_peer = df_pro_bot_forecasts.copy()\n",
    "df_bot_vs_pro_peer = df_bot_vs_pro_peer[['pro_median', 'resolution', 'question_weight', 'bot_question_id']]\n",
    "\n",
    "# Calculate peer score for each bot\n",
    "for bot in all_bots:\n",
    "    # Calculate Head-to-head score based on the condition\n",
    "    peer_score = np.where(\n",
    "        df_pro_bot_forecasts['resolution'] == 'yes',\n",
    "        np.log(df_pro_bot_forecasts[bot] / df_pro_bot_forecasts['pro_median']),\n",
    "        np.log((1 - df_pro_bot_forecasts[bot]) / (1 - df_pro_bot_forecasts['pro_median']))\n",
    "    )\n",
    "\n",
    "    # Add the calculated peer score to the new DataFrame\n",
    "    df_bot_vs_pro_peer[bot] = 100 * peer_score\n",
    "\n",
    "# Calculate Head-to-head score for bot_team\n",
    "peer_score = np.where(\n",
    "    df_pro_bot_forecasts['resolution'] == 'yes',\n",
    "    np.log(df_pro_bot_forecasts['bot_median'] / df_pro_bot_forecasts['pro_median']),\n",
    "    np.log((1 - df_pro_bot_forecasts['bot_median']) / (1 - df_pro_bot_forecasts['pro_median']))\n",
    ")\n",
    "\n",
    "# Add the calculated peer score to the new DataFrame\n",
    "df_bot_vs_pro_peer[\"bot_team_median\"] = 100 * peer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = df_bot_vs_pro_peer.sum(axis=0)\n",
    "# remove resolution, question_weight, bot_question_id from total scores\n",
    "total_scores = total_scores.drop(['resolution', 'question_weight', 'bot_question_id'])\n",
    "\n",
    "# First pivot to long format - each row will be a question-forecaster pair\n",
    "df_long = df_bot_vs_pro_peer.melt(\n",
    "    id_vars=['bot_question_id', 'question_weight', 'resolution'],\n",
    "    var_name='forecaster',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Drop any rows where score is NaN\n",
    "df_long = df_long.dropna(subset=['score'])\n",
    "\n",
    "# Cast question_weight as numeric\n",
    "df_long['question_weight'] = pd.to_numeric(df_long['question_weight'], errors='coerce')\n",
    "\n",
    "# Group first, then do the multiplication and sum\n",
    "weighted_scores = df_long.groupby('forecaster').apply(lambda x: (x['score'] * x['question_weight']).sum(axis=0))\n",
    "\n",
    "# Calculate number of questions answered by each bot\n",
    "num_questions = df_long.groupby('forecaster')['bot_question_id'].nunique()\n",
    "#num_weighted_questions = df_bot_vs_pro_peer.mul(df_pro_bot_forecasts['question_weight'], axis=0).apply(lambda col: col[col.notna() & col.apply(np.isreal)].count())\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "results = pd.DataFrame({\n",
    "    'Peer_vs_Pro': total_scores,\n",
    "    'Count': num_questions\n",
    "})\n",
    "\n",
    "weighted_results = pd.DataFrame({\n",
    "    'W_Peer_vs_Pro': weighted_scores,\n",
    "    'Count': num_questions\n",
    "})\n",
    "\n",
    "df_bot_vs_pro_leaderboard = results.sort_values(by='Peer_vs_Pro', ascending=False)\n",
    "df_bot_vs_pro_weighted_leaderboard = weighted_results.sort_values(by='W_Peer_vs_Pro', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pro_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename question_id to pro_question_id in df_pro_baseline and filter to only baseline score_type\n",
    "df_pro_baseline = df_pro_scores.rename(columns={'question_id': 'pro_question_id'})[df_pro_scores['score_type'] == 'spot_baseline']\n",
    "df_pro_baseline = df_pro_baseline[['pro_question_id', 'forecaster', 'score']]\n",
    "\n",
    "# Now make it wide! forecaster = columns; score = values; index = pro_question_id\n",
    "df_pro_baseline_wide = df_pro_baseline.pivot(index='pro_question_id', columns='forecaster', values='score').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tXKRpXAVHMRt"
   },
   "outputs": [],
   "source": [
    "# @title Create df_pro_bot_baseline_leaderboard, df_pro_bot_baseline_weighted_leaderboard\n",
    "\n",
    "df_pro_bot_baseline_weights = pd.merge(\n",
    "    df_pro_bot_resolved_questions,\n",
    "    df_bot_baseline_wide,\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_baseline_weights = pd.merge(\n",
    "    df_pro_bot_baseline_weights,\n",
    "    df_pro_baseline_wide[['pro_question_id', 'pro_median']],\n",
    "    on='pro_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove rows where pro_question_id is NaN (only want overlapping questions here)\n",
    "df_pro_bot_baseline_weights = df_pro_bot_baseline_weights.dropna(subset=['pro_question_id'])\n",
    "\n",
    "# Create a list of columns to keep\n",
    "forecaster_cols = ['pro_median'] + [col for col in df_pro_bot_baseline_weights.columns if col in all_bots]\n",
    "df_filtered = df_pro_bot_baseline_weights[forecaster_cols]\n",
    "\n",
    "# Calculate the sum for each forecaster\n",
    "forecaster_scores = df_filtered.sum()\n",
    "forecaster_weighted_scores = df_filtered.mul(df_pro_bot_baseline_weights['question_weight'], axis=0).sum()\n",
    "\n",
    "question_counts = df_filtered.notna().sum()\n",
    "question_weighted_counts = df_filtered.notna().mul(df_pro_bot_baseline_weights['question_weight'], axis=0).sum()\n",
    "\n",
    "# Create a DataFrame for the leaderboard\n",
    "leaderboard = pd.DataFrame({\n",
    "    'Forecaster': forecaster_scores.index,\n",
    "    'Baseline': forecaster_scores.values,\n",
    "    'Count': question_counts.values\n",
    "})\n",
    "\n",
    "# Create a DataFrame for the leaderboard\n",
    "weighted_leaderboard = pd.DataFrame({\n",
    "    'Forecaster': forecaster_weighted_scores.index,\n",
    "    'Weighted_Baseline': forecaster_weighted_scores.values,\n",
    "    'Count': question_counts.values,\n",
    "    'Weighted Count': question_weighted_counts.values\n",
    "})\n",
    "\n",
    "# Sort the leaderboard by score in descending order\n",
    "leaderboard = leaderboard.sort_values('Baseline', ascending=False).reset_index(drop=True)\n",
    "weighted_leaderboard = weighted_leaderboard.sort_values('Weighted_Baseline', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add a 'Rank' column\n",
    "leaderboard['Rank'] = leaderboard.index + 1\n",
    "weighted_leaderboard['Rank'] = weighted_leaderboard.index + 1\n",
    "\n",
    "# Reorder columns to have Rank first\n",
    "leaderboard = leaderboard[['Rank', 'Forecaster', 'Baseline', 'Count']]\n",
    "weighted_leaderboard = weighted_leaderboard[['Rank', 'Forecaster', 'Weighted_Baseline', 'Count', 'Weighted Count']]\n",
    "\n",
    "#leaderboard\n",
    "weighted_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ANWagKKaJH4-"
   },
   "outputs": [],
   "source": [
    "# @title function t_critical_value\n",
    "\n",
    "def t_critical_value(df):\n",
    "    # Dictionary containing t-values for 95% confidence interval (2-tailed)\n",
    "    t_table = {\n",
    "        1: 12.706, 2: 4.303, 3: 3.182, 4: 2.776, 5: 2.571,\n",
    "        6: 2.447, 7: 2.365, 8: 2.306, 9: 2.262, 10: 2.228,\n",
    "        11: 2.201, 12: 2.179, 13: 2.160, 14: 2.145, 15: 2.131,\n",
    "        16: 2.120, 17: 2.110, 18: 2.101, 19: 2.093, 20: 2.086,\n",
    "        21: 2.080, 22: 2.074, 23: 2.069, 24: 2.064, 25: 2.060,\n",
    "        26: 2.056, 27: 2.052, 28: 2.048, 29: 2.045, 30: 2.042,\n",
    "        40: 2.021, 50: 2.009, 60: 2.000, 70: 1.994, 80: 1.990,\n",
    "        90: 1.987, 100: 1.984, 1000: 1.962, float('inf'): 1.960\n",
    "    }\n",
    "\n",
    "    # Check if df is in the table\n",
    "    if df in t_table:\n",
    "        return t_table[df]\n",
    "\n",
    "    # If df is not in the table, find the closest lower df\n",
    "    lower_df = max(key for key in t_table.keys() if key <= df)\n",
    "\n",
    "    # If df is between two values, perform linear interpolation\n",
    "    if lower_df < df:\n",
    "        upper_df = min(key for key in t_table.keys() if key > df)\n",
    "        lower_t = t_table[lower_df]\n",
    "        upper_t = t_table[upper_df]\n",
    "\n",
    "        # Linear interpolation\n",
    "        t_value = lower_t + (upper_t - lower_t) * (df - lower_df) / (upper_df - lower_df)\n",
    "        return round(t_value, 3)\n",
    "\n",
    "    return t_table[lower_df]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "#print(f\"Critical t-value for df=3.5: {t_critical_value(3.5)}\")\n",
    "#print(f\"Critical t-value for df=35: {t_critical_value(35)}\")\n",
    "#print(f\"Critical t-value for df=200: {t_critical_value(200)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TK: Sanity check. Look at MWG and RyansAGI individual questions - some weird resolutions where the pros were caught off guard\n",
    "# So maybe the bots are really (almost all) this much worse than pros? Looking at the table below.\n",
    "# Those \"unexpected\" resolutions what kept the bots in the game last quarter, maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many questions in df_bot_vs_pro_peer\n",
    "df_bot_vs_pro_peer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGNedTHmU-Bm",
    "outputId": "a7935679-8993-4329-d05d-fd701c4b77a8"
   },
   "outputs": [],
   "source": [
    "# @title Weighted head-to-head, T test\n",
    "\n",
    "\"\"\"\n",
    "df_W_leaderboard: A leaderboard based on df_bot_vs_pro_peer with question\n",
    "weighting and the calculations for doing a weighted T test\n",
    "\"\"\"\n",
    "\n",
    "forecaster_weighted_scores = forecaster_weighted_scores.fillna(0)\n",
    "\n",
    "# Cast weights as numeric\n",
    "df_bot_vs_pro_peer['question_weight'] = pd.to_numeric(df_bot_vs_pro_peer['question_weight'], errors='coerce')\n",
    "\n",
    "df_W_leaderboard = pd.DataFrame()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "for bot in all_bots:\n",
    "\n",
    "  # T test\n",
    "  df3 = df_bot_vs_pro_peer.copy()\n",
    "  df3 = df_bot_vs_pro_peer[[bot, 'question_weight']]\n",
    "  #df3['question_weight'] = 1\n",
    "  df3 = df3.dropna()\n",
    "  df3 = df3.reset_index(drop=True)\n",
    "  weighted_score = (df3[bot] * df3['question_weight']).sum()\n",
    "  weighted_count = df3['question_weight'].sum()\n",
    "  weighted_average = weighted_score / weighted_count\n",
    "  weighted_std_dev = np.sqrt(((df3[bot] - weighted_average) ** 2 * df3['question_weight']).sum() / (weighted_count - 1))\n",
    "  std_error = weighted_std_dev / np.sqrt(weighted_count)\n",
    "  t_statistic = (weighted_average - 0) / std_error\n",
    "  if weighted_count > 2:\n",
    "    t_crit = t_critical_value(weighted_count - 1)\n",
    "    # 95% confidence that the true mean is between lower and upper bounds\n",
    "    upper_bound = weighted_average + t_crit * std_error\n",
    "    lower_bound = weighted_average - t_crit * std_error\n",
    "    # 1.96 sigma for a normal Gaussian correponds to 97.5th percentile\n",
    "    # Normalization below\n",
    "    cdf = norm.cdf(t_statistic * 1.96 / t_crit)\n",
    "  else:\n",
    "    t_crit = np.nan\n",
    "    upper_bound = np.nan\n",
    "    lower_bound = np.nan\n",
    "    cdf = np.nan\n",
    "\n",
    "  df_W_leaderboard.loc[bot, 'W_score'] = weighted_score\n",
    "  df_W_leaderboard.loc[bot, 'W_count'] = weighted_count\n",
    "  df_W_leaderboard.loc[bot, 'W_ave'] = weighted_average\n",
    "  df_W_leaderboard.loc[bot, 'W_stdev'] = weighted_std_dev\n",
    "  df_W_leaderboard.loc[bot, 'std_err'] = std_error\n",
    "  df_W_leaderboard.loc[bot, 't_stat'] = t_statistic\n",
    "  df_W_leaderboard.loc[bot, 't_crit'] = t_crit\n",
    "  df_W_leaderboard.loc[bot, 'upper_bound'] = upper_bound\n",
    "  df_W_leaderboard.loc[bot, 'lower_bound'] = lower_bound\n",
    "  df_W_leaderboard.loc[bot, 'cdf'] = cdf * 100\n",
    "\n",
    "df_W_leaderboard = df_W_leaderboard.sort_values(by='W_score', ascending=False)\n",
    "\n",
    "df_W_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d_ZdL0A0qTz",
    "outputId": "e30ee8fb-0faf-45ae-974e-d4af282e0252"
   },
   "outputs": [],
   "source": [
    "# @title Weighted Bot Peer, T test\n",
    "\n",
    "df_W_bot_peer_leaderboard = pd.DataFrame()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "forecaster_weighted_scores = forecaster_weighted_scores.fillna(0)\n",
    "\n",
    "# OMIT bot_median column for this bit\n",
    "df_bot_peer_wide_b = df_bot_peer_wide.drop('bot_median', axis=1)\n",
    "df_bot_peer = df_bot_peer[df_bot_peer['forecaster'] != 'bot_median']\n",
    "\n",
    "# Must use only forecasters who are in df_bot_peer_wide (i.e. some in all_bots ONLY responded to questions with pro benchmark; must exclude those)\n",
    "bots_for_peer = np.array(list(set(df_bot_peer['forecaster'])))\n",
    "\n",
    "for bot in bots_for_peer:\n",
    "\n",
    "  # T test\n",
    "  df3 = df_bot_peer_wide_b[[bot, 'question_weight']]\n",
    "  df3 = df3.dropna()\n",
    "  df3 = df3.reset_index(drop=True)\n",
    "  weighted_score = (df3[bot] * df3['question_weight']).sum()\n",
    "  weighted_count = df3['question_weight'].sum()\n",
    "  weighted_average = weighted_score / weighted_count\n",
    "  weighted_std_dev = np.sqrt(((df3[bot] - weighted_average) ** 2 * df3['question_weight']).sum() / (weighted_count - 1))\n",
    "  std_error = weighted_std_dev / np.sqrt(weighted_count)\n",
    "  t_statistic = (weighted_average - 0) / std_error\n",
    "  if weighted_count > 2:\n",
    "    t_crit = t_critical_value(weighted_count - 1)\n",
    "    # 95% confidence that the true mean is between lower and upper bounds\n",
    "    upper_bound = weighted_average + t_crit * std_error\n",
    "    lower_bound = weighted_average - t_crit * std_error\n",
    "    # 1.96 sigma for a normal Gaussian correponds to 97.5th percentile\n",
    "    # Normalization below\n",
    "    cdf = norm.cdf(t_statistic * 1.96 / t_crit)\n",
    "  else:\n",
    "    t_crit = np.nan\n",
    "    upper_bound = np.nan\n",
    "    lower_bound = np.nan\n",
    "    cdf = np.nan\n",
    "\n",
    "  df_W_leaderboard.loc[bot, 'W_score'] = weighted_score\n",
    "  df_W_leaderboard.loc[bot, 'W_count'] = weighted_count\n",
    "  df_W_leaderboard.loc[bot, 'W_ave'] = weighted_average\n",
    "  df_W_leaderboard.loc[bot, 'W_stdev'] = weighted_std_dev\n",
    "  df_W_leaderboard.loc[bot, 'std_err'] = std_error\n",
    "  df_W_leaderboard.loc[bot, 't_stat'] = t_statistic\n",
    "  df_W_leaderboard.loc[bot, 't_crit'] = t_crit\n",
    "  df_W_leaderboard.loc[bot, 'upper_bound'] = upper_bound\n",
    "  df_W_leaderboard.loc[bot, 'lower_bound'] = lower_bound\n",
    "  df_W_leaderboard.loc[bot, 'cdf'] = cdf * 100\n",
    "\n",
    "df_W_leaderboard.sort_values(by='W_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "88QO8eyW6T_T",
    "outputId": "e83d6794-13a2-454d-cb70-0a38b065d9e7"
   },
   "outputs": [],
   "source": [
    "# @title Histogram of bot\n",
    "\n",
    "# Extract the 'mf-bot-1' column and remove NaN values\n",
    "#scores = df_bot_vs_pro_peer['FJ_Researcher01+bot'].dropna()\n",
    "\n",
    "name = 'mf-bot-1'\n",
    "\n",
    "scores = df_bot_peer_wide[name].dropna()\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(scores, bins=30, density=True, alpha=0.7, color='skyblue')\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mu, std = norm.fit(scores)\n",
    "\n",
    "# Plot the PDF of the fitted normal distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f\"Histogram of {name} Scores with Fitted Gaussian\", fontsize=16)\n",
    "plt.xlabel(\"Score\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "\n",
    "# Add text box with distribution parameters\n",
    "textstr = f'$\\mu={mu:.2f}$\\n$\\sigma={std:.2f}$'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bot_peer_wide.shape\n",
    "\n",
    "# Print all_bots in alphabetical order\n",
    "df_bot_peer_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxVJxrCpuXV_",
    "outputId": "3df39cbc-b594-40e1-d08f-1b0e9736d6ec"
   },
   "outputs": [],
   "source": [
    "# @title Bootstrapping\n",
    "\n",
    "# Drop 'bot_median from all_bots list\n",
    "all_bots_wo_median = np.delete(all_bots, np.where(all_bots == 'bot_median')[0][0])\n",
    "df_bot_peer_wide_wo_median = df_bot_peer_wide.drop('bot_median', axis=1)\n",
    "\n",
    "def weighted_bootstrap_analysis(df_bot_peer_wide, bots, NUM, ITER):\n",
    "    # Function to perform a single bootstrap iteration\n",
    "    def single_bootstrap(df):\n",
    "        # Weighted sampling of questions\n",
    "        sampled_df = df.sample(n=NUM, weights='question_weight', replace=True)\n",
    "        # Calculate total weighted score for each bot\n",
    "        return sampled_df[bots].sum()\n",
    "\n",
    "    # Perform bootstrap ITER times\n",
    "    bootstrap_results = [single_bootstrap(df_bot_peer_wide) for _ in range(ITER)]\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(bootstrap_results)\n",
    "\n",
    "    # Calculate confidence intervals and median\n",
    "    ci_low = results_df.quantile(0.025)\n",
    "    ci_10 = results_df.quantile(0.1)\n",
    "    ci_high = results_df.quantile(0.975)\n",
    "    ci_90 = results_df.quantile(0.9)\n",
    "    median = results_df.median()\n",
    "\n",
    "    # Create output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        '2.5% CI': ci_low,\n",
    "        '10% CI': ci_10,\n",
    "        'Median': median,\n",
    "        '90% CI': ci_90,\n",
    "        '97.5% CI': ci_high\n",
    "    })\n",
    "\n",
    "    # Sort by median descending\n",
    "    output_df = output_df.sort_values('Median', ascending=False)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "NUM = round(df_bot_peer_wide['question_weight'].sum())\n",
    "ITER = 1000\n",
    "\n",
    "result_df = weighted_bootstrap_analysis(df_bot_peer_wide_wo_median, all_bots_wo_median, NUM, ITER)\n",
    "average_df = result_df / NUM\n",
    "\n",
    "print(f'BOT LEADERBOARD\\n\\n')\n",
    "df_rounded = average_df.round(1)\n",
    "df_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "MXAev2sNXdbZ",
    "outputId": "eebb723f-5494-4b89-cf0d-efa5b1626cb7"
   },
   "outputs": [],
   "source": [
    "NUM = round(df_bot_vs_pro_peer['question_weight'].sum())\n",
    "ITER = 1000\n",
    "\n",
    "result_df = weighted_bootstrap_analysis(df_bot_vs_pro_peer, all_bots, NUM, ITER)\n",
    "average_df = result_df / NUM\n",
    "\n",
    "print(f'\\n\\n\\nHEAD-TO-HEAD LEADERBOARD\\n\\n')\n",
    "#df_rounded = result_df.round(0).astype(int)\n",
    "df_rounded = average_df.round(1)\n",
    "\n",
    "df_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Check specific bot records\n",
    "\n",
    "bot_name = 'annabot'\n",
    "\n",
    "df_bot = df_bot_peer_wide[['bot_question_id', 'question_weight', bot_name]]\n",
    "df_bot = df_bot.dropna()\n",
    "df_bot = df_bot.reset_index(drop=True)\n",
    "\n",
    "df_bot['weighted_score'] = df_bot[bot_name] * df_bot['question_weight']\n",
    "\n",
    "weighted_score = df_bot['weighted_score'].sum()\n",
    "\n",
    "print(f\"Weighted score for {bot_name}: {weighted_score}\")\n",
    "\n",
    "total_score = df_bot[bot_name].sum()\n",
    "\n",
    "print(f\"Total score for {bot_name}: {total_score}\\n\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size (optional)\n",
    "plt.hist(df_bot[bot_name], bins=10, edgecolor='black')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f'Histogram of Scores for {bot_name}')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add grid lines (optional)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values of pro_question_id in df_pro_bot_resolved_questions\n",
    "df_pro_bot_resolved_questions['pro_question_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7W8JXutv2ks",
    "outputId": "5e7053d3-2124-42b7-bd53-48a40a53caf2"
   },
   "outputs": [],
   "source": [
    "# @title Weighted Bot Only Peer, T test\n",
    "\n",
    "df_W_bot_only_peer_leaderboard = pd.DataFrame()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "# To choose our top bot team, we only use the questions for which there is no Pro benchmark.\n",
    "no_pro_benchmark = df_pro_bot_resolved_questions[df_pro_bot_resolved_questions['pro_question_id'].isna()]['bot_question_id']\n",
    "\n",
    "df_bot_only_peer = df_bot_peer[df_bot_peer['bot_question_id'].isin(no_pro_benchmark)]\n",
    "df_bot_only_peer_wide = make_wide(df_bot_only_peer)\n",
    "\n",
    "for bot in df_bot_only_peer['forecaster'].unique():\n",
    "\n",
    "  # T test\n",
    "  df3 = df_bot_only_peer_wide[[bot, 'question_weight']]\n",
    "  df3 = df3.dropna()\n",
    "  df3 = df3.reset_index(drop=True)\n",
    "  weighted_score = (df3[bot] * df3['question_weight']).sum()\n",
    "  weighted_count = df3['question_weight'].sum()\n",
    "  weighted_average = weighted_score / weighted_count\n",
    "  weighted_std_dev = np.sqrt(((df3[bot] - weighted_average) ** 2 * df3['question_weight']).sum() / (weighted_count - 1))\n",
    "  std_error = weighted_std_dev / np.sqrt(weighted_count)\n",
    "  t_statistic = (weighted_average - 0) / std_error\n",
    "  if weighted_count > 2:\n",
    "    t_crit = t_critical_value(weighted_count - 1)\n",
    "    # 95% confidence that the true mean is between lower and upper bounds\n",
    "    upper_bound = weighted_average + t_crit * std_error\n",
    "    lower_bound = weighted_average - t_crit * std_error\n",
    "    # 1.96 sigma for a normal Gaussian correponds to 97.5th percentile\n",
    "    # Normalization below\n",
    "    cdf = norm.cdf(t_statistic * 1.96 / t_crit)\n",
    "  else:\n",
    "    t_crit = np.nan\n",
    "    upper_bound = np.nan\n",
    "    lower_bound = np.nan\n",
    "    cdf = np.nan\n",
    "\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'W_score'] = weighted_score\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'W_count'] = weighted_count\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'W_ave'] = weighted_average\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'W_stdev'] = weighted_std_dev\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'std_err'] = std_error\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 't_stat'] = t_statistic\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 't_crit'] = t_crit\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'upper_bound'] = upper_bound\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'lower_bound'] = lower_bound\n",
    "  df_W_bot_only_peer_leaderboard.loc[bot, 'cdf'] = cdf * 100\n",
    "\n",
    "df_W_bot_only_peer_leaderboard.sort_values(by='lower_bound', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the lower_bound column in descending order\n",
    "sorted_df = df_W_bot_only_peer_leaderboard.sort_values(by='lower_bound', ascending=False)\n",
    "\n",
    "# exclude bot median for purposes of bot teaming\n",
    "sorted_df = sorted_df.drop('bot_median', errors='ignore') \n",
    "\n",
    "# Get the top 10 bot names\n",
    "top_10_bots = sorted_df.index[:10].tolist()\n",
    "\n",
    "# Print the list of top 10 bots\n",
    "print(\"Top 10 bots:\")\n",
    "for i, bot in enumerate(top_10_bots, 1):\n",
    "    print(f\"{i}. {bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x6e1kZl12qFZ"
   },
   "outputs": [],
   "source": [
    "# @title Calculate df_bot_team_forecasts\n",
    "\n",
    "df_bot_team_forecasts = pd.merge(\n",
    "    df_bot_forecasts,\n",
    "    df_pro_bot_resolved_questions[['bot_question_id', 'question_weight', 'resolution']],\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "columns_to_keep = ['bot_question_id', 'question_weight', 'resolution'] + top_10_bots\n",
    "\n",
    "# Filter the DataFrame to keep only the specified columns\n",
    "df_bot_team_forecasts = df_bot_team_forecasts[columns_to_keep]\n",
    "\n",
    "#print(df_bot_team_forecasts)\n",
    "\n",
    "# Function to calculate median forecast for a given number of bots\n",
    "def calculate_median_forecast(df, bots):\n",
    "    return df[bots].median(axis=1)\n",
    "\n",
    "# Calculate and add median forecasts for 2 to 10 bots\n",
    "for i in range(1, 11):\n",
    "    bots_subset = top_10_bots[:i]\n",
    "    column_name = f'median_forecast_{i}_bots'\n",
    "    df_bot_team_forecasts[column_name] = calculate_median_forecast(df_bot_team_forecasts, bots_subset)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df_bot_team_forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-FedHpWV_1v",
    "outputId": "7327c204-c501-4dfb-bdfb-176606c96dc4"
   },
   "outputs": [],
   "source": [
    "# @title Calculate the baseline scores for each team size\n",
    "\n",
    "teams = ['median_forecast_1_bots',\n",
    "         'median_forecast_2_bots',\n",
    "         'median_forecast_3_bots',\n",
    "         'median_forecast_4_bots',\n",
    "         'median_forecast_5_bots',\n",
    "         'median_forecast_6_bots',\n",
    "         'median_forecast_7_bots',\n",
    "         'median_forecast_8_bots',\n",
    "         'median_forecast_9_bots',\n",
    "         'median_forecast_10_bots']\n",
    "\n",
    "def calculate_weighted_scores(df_bot_team_forecasts, teams):\n",
    "    # Initialize a dictionary to store the weighted scores for each team\n",
    "    team_scores = {team: 0 for team in teams}\n",
    "\n",
    "    # Iterate through each row (question) in the dataframe\n",
    "    for _, row in df_bot_team_forecasts.iterrows():\n",
    "        resolution = row['resolution']\n",
    "        question_weight = row['question_weight']\n",
    "\n",
    "        # Calculate the baseline score for each team\n",
    "        for team in teams:\n",
    "            forecast = row[team]\n",
    "\n",
    "            # Calculate the baseline score based on the resolution\n",
    "            if resolution == 'yes':\n",
    "                baseline_score = np.log2(forecast / 0.5)\n",
    "            elif resolution == 'no':\n",
    "                baseline_score = np.log2((1 - forecast) / 0.5)\n",
    "            else:\n",
    "                # Skip if resolution is neither 0 nor 1\n",
    "                continue\n",
    "\n",
    "            # Calculate the weighted score and add it to the team's total\n",
    "            weighted_score = baseline_score * question_weight\n",
    "            team_scores[team] += weighted_score\n",
    "\n",
    "    # Convert the dictionary to a pandas Series for easier handling\n",
    "    return pd.Series(team_scores)\n",
    "\n",
    "weighted_scores = calculate_weighted_scores(df_bot_team_forecasts, teams)\n",
    "print(weighted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3TTBVWoZVzU",
    "outputId": "0eb32f2c-09c6-4a15-e81a-bee353b1bccf"
   },
   "outputs": [],
   "source": [
    "# @title Weighted team-vs-pro\n",
    "\n",
    "# We have our top bot team members.\n",
    "# Calculate their median forecast on the pro_bot questions.\n",
    "# Create df with bot_question_id, forecasts, resolution, weights\n",
    "# Calculate the head-to-head score\n",
    "\n",
    "top_bot_team = top_10_bots[:9]\n",
    "#print(top_bot_team)\n",
    "\n",
    "df_top_bot_forecasts = df_bot_forecasts[['bot_question_id'] + top_bot_team]\n",
    "df_top_bot_forecasts['bot_team_median'] = df_top_bot_forecasts[top_bot_team].median(axis=1)\n",
    "#print(df_top_bot_forecasts)\n",
    "\n",
    "df_pro_median = df_pro_forecasts[['pro_question_id', 'pro_median']]\n",
    "\n",
    "df_top_bot_pro_forecasts = pd.merge(\n",
    "    df_pro_bot_resolved_questions,\n",
    "    df_top_bot_forecasts[['bot_question_id', 'bot_team_median']],\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_top_bot_pro_forecasts = pd.merge(\n",
    "    df_top_bot_pro_forecasts,\n",
    "    df_pro_median,\n",
    "    on='pro_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter to only those rows where pro_median is not NA\n",
    "df_top_bot_pro_forecasts = df_top_bot_pro_forecasts.dropna(subset=['pro_median'])\n",
    "\n",
    "def calculate_head_to_head(row):\n",
    "    if row['resolution'] == 'yes':\n",
    "        return 100* np.log(row['bot_team_median'] / row['pro_median'])\n",
    "    elif row['resolution'] == 'no':\n",
    "        return 100* np.log((1 - row['bot_team_median']) / (1 - row['pro_median']))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Add the head_to_head column\n",
    "df_top_bot_pro_forecasts['head_to_head'] = df_top_bot_pro_forecasts.apply(calculate_head_to_head, axis=1)\n",
    "\n",
    "df_top_bot_pro_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted score for each row\n",
    "df_top_bot_pro_forecasts['weighted_score'] = df_top_bot_pro_forecasts['head_to_head'] * df_top_bot_pro_forecasts['question_weight']\n",
    "\n",
    "# Calculate the total weighted score\n",
    "total_weighted_score = df_top_bot_pro_forecasts['weighted_score'].sum()\n",
    "\n",
    "# Calculate the sum of weights\n",
    "total_weight = df_top_bot_pro_forecasts['question_weight'].sum()\n",
    "\n",
    "# Calculate the weighted total score\n",
    "weighted_total_score = total_weighted_score / total_weight\n",
    "\n",
    "print(f\"Weighted Total Score: {weighted_total_score:.4f}\")\n",
    "\n",
    "# TK: Interval? Might come later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "JlU9zyqn26Rl",
    "outputId": "ac54d636-670b-4a8f-aea9-402679efacf9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Assuming df_top_bot_pro_forecasts is already defined and contains the 'head_to_head' column\n",
    "\n",
    "# Extract the 'head_to_head' data\n",
    "data = df_top_bot_pro_forecasts['head_to_head']\n",
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(data, bins=30, density=True, alpha=0.7, color='skyblue')\n",
    "\n",
    "# Generate points for the fitted Gaussian curve\n",
    "x = np.linspace(min(data), max(data), 100)\n",
    "y = norm.pdf(x, mean, std)\n",
    "\n",
    "# Plot the fitted Gaussian curve\n",
    "plt.plot(x, y, 'r-', linewidth=2, label='Fitted Gaussian')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Bot Team Head-to-Head Scores vs Pros')\n",
    "plt.xlabel('Head-to-Head Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Add text annotation for the mean\n",
    "#plt.text(0.95, 0.95, f'Mean: {mean:.2f}', transform=plt.gca().transAxes, verticalalignment='top', horizontalalignment='right')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the average\n",
    "print(f\"The average of 'head_to_head' is: {mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1qC4m2VefLe",
    "outputId": "2f110b55-caf6-4ea8-9dfe-b746c3e4d892"
   },
   "outputs": [],
   "source": [
    "\n",
    "df3 = df_top_bot_pro_forecasts.copy()\n",
    "\n",
    "df3 = df3[['head_to_head', 'question_weight']]\n",
    "df3 = df3.dropna()\n",
    "df3 = df3.reset_index(drop=True)\n",
    "weighted_score = (df3['head_to_head'] * df3['question_weight']).sum()\n",
    "weighted_count = df3['question_weight'].sum()\n",
    "weighted_average = weighted_score / weighted_count\n",
    "weighted_std_dev = np.sqrt(((df3['head_to_head'] - weighted_average) ** 2 * df3['question_weight']).sum() / (weighted_count - 1))\n",
    "std_error = weighted_std_dev / np.sqrt(weighted_count)\n",
    "t_statistic = (weighted_average - 0) / std_error\n",
    "if weighted_count > 2:\n",
    "  t_crit = t_critical_value(weighted_count - 1)\n",
    "  # 95% confidence that the true mean is between lower and upper bounds\n",
    "  upper_bound = weighted_average + t_crit * std_error\n",
    "  lower_bound = weighted_average - t_crit * std_error\n",
    "  # 1.96 sigma for a normal Gaussian correponds to 97.5th percentile\n",
    "  # Normalization below\n",
    "  cdf = norm.cdf(t_statistic * 1.96 / t_crit)\n",
    "else:\n",
    "  t_crit = np.NaN\n",
    "  upper_bound = np.NaN\n",
    "  lower_bound = np.NaN\n",
    "  cdf = np.NaN\n",
    "\n",
    "df_bot_team_h2h = pd.DataFrame()\n",
    "\n",
    "df_bot_team_h2h.loc[bot, 'W_score'] = weighted_score\n",
    "df_bot_team_h2h.loc[bot, 'W_count'] = weighted_count\n",
    "df_bot_team_h2h.loc[bot, 'W_ave'] = weighted_average\n",
    "df_bot_team_h2h.loc[bot, 'W_stdev'] = weighted_std_dev\n",
    "df_bot_team_h2h.loc[bot, 'std_err'] = std_error\n",
    "df_bot_team_h2h.loc[bot, 't_stat'] = t_statistic\n",
    "df_bot_team_h2h.loc[bot, 't_crit'] = t_crit\n",
    "df_bot_team_h2h.loc[bot, 'upper_bound'] = upper_bound\n",
    "df_bot_team_h2h.loc[bot, 'lower_bound'] = lower_bound\n",
    "df_bot_team_h2h.loc[bot, 'cdf'] = cdf * 100\n",
    "\n",
    "print(df_bot_team_h2h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0I0myCHpl7FT",
    "outputId": "bcc45b9a-f328-4f0c-ef98-a7620af7e358"
   },
   "outputs": [],
   "source": [
    "df_sorted = df_top_bot_pro_forecasts.sort_values(by='head_to_head')\n",
    "\n",
    "df_top5 = df_sorted.head(5)\n",
    "df_bottom5 = df_sorted.tail(5)\n",
    "\n",
    "print(\"Top 5:\")\n",
    "\n",
    "df_top5[['title', 'bot_team_median', 'pro_median', 'resolution', 'head_to_head']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBottom 5:\")\n",
    "\n",
    "df_bottom5[['title', 'bot_team_median', 'pro_median', 'resolution', 'head_to_head']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast df_top_bot_pro_forecasts['resolution'] as string - idk why this is necessary but it is\n",
    "df_top_bot_pro_forecasts['resolution'] = df_top_bot_pro_forecasts['resolution'].astype(pd.StringDtype())\n",
    "df_top_bot_pro_forecasts['resolution'] = df_top_bot_pro_forecasts['resolution'].map({'yes': 1, 'no': 0})\n",
    "df_top_bot_pro_forecasts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "BjNQ4IND6Ct7",
    "outputId": "c0ec1316-ef4e-4bd1-875d-148b65ba0114"
   },
   "outputs": [],
   "source": [
    "# TK: Make this nice calibration curve with error bars\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Function to plot calibration curve\n",
    "def plot_calibration_curve(df, column_name, label, color):\n",
    "\n",
    "    # Extract actual outcomes and predictions\n",
    "    y_true = df['resolution']\n",
    "    y_pred = df[column_name]\n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5, strategy='quantile')\n",
    "\n",
    "    # Plot calibration curve\n",
    "    plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label=label, color=color)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "\n",
    "# Plot calibration curves for bot_team_median and pro_median\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts, 'bot_team_median', 'Bot Team Median', 'blue')\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts, 'pro_median', 'Pro Median', 'red')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Predicted probability', fontsize=12)\n",
    "plt.ylabel('Actual Outcome Probability', fontsize=12)\n",
    "plt.title('Calibration Curve: Bot Team Median vs Pro Median', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add diagonal line for perfect calibration\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPPgorXB7omi",
    "outputId": "24571b16-50b7-4e51-cd3d-420c15c7fe42"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_top_bot_pro_forecasts is already defined\n",
    "\n",
    "def calculate_confidence(predictions, outcomes):\n",
    "    \"\"\"\n",
    "    Calculate over- or under-confidence for a set of predictions.\n",
    "\n",
    "    :param predictions: Series of predicted probabilities\n",
    "    :param outcomes: Series of actual outcomes (0 or 1)\n",
    "    :return: Confidence score (positive for overconfidence, negative for underconfidence)\n",
    "    \"\"\"\n",
    "    # Bin predictions into 10 equally spaced bins\n",
    "    bins = pd.cut(predictions, bins=10)\n",
    "\n",
    "    # Calculate mean prediction and actual outcome for each bin\n",
    "    grouped = pd.DataFrame({'prediction': predictions, 'outcome': outcomes}).groupby(bins)\n",
    "    mean_prediction = grouped['prediction'].mean()\n",
    "    mean_outcome = grouped['outcome'].mean()\n",
    "\n",
    "    # Calculate the difference between mean prediction and mean outcome\n",
    "    confidence_diff = mean_prediction - mean_outcome\n",
    "\n",
    "    # Return the average difference (excluding NaN values)\n",
    "    return np.nanmean(confidence_diff)\n",
    "\n",
    "# Calculate confidence scores for bot_team_median and pro_median\n",
    "bot_confidence = calculate_confidence(df_top_bot_pro_forecasts['bot_team_median'], df_top_bot_pro_forecasts['resolution'])\n",
    "pro_confidence = calculate_confidence(df_top_bot_pro_forecasts['pro_median'], df_top_bot_pro_forecasts['resolution'])\n",
    "\n",
    "print(f\"Bot team confidence score: {bot_confidence:.4f}\")\n",
    "print(f\"Pro team confidence score: {pro_confidence:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "def interpret_confidence(score):\n",
    "    if score > 0:\n",
    "        return f\"Overconfident by {score:.4f}\"\n",
    "    elif score < 0:\n",
    "        return f\"Underconfident by {abs(score):.4f}\"\n",
    "    else:\n",
    "        return \"Perfectly calibrated\"\n",
    "\n",
    "print(f\"Bot team is {interpret_confidence(bot_confidence)}\")\n",
    "print(f\"Pro team is {interpret_confidence(pro_confidence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N26JZjCV9_jc",
    "outputId": "eacb7626-54d0-47c7-8f21-48e95e709564"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_top_bot_pro_forecasts is already loaded\n",
    "# If not, you would load it here:\n",
    "# df_top_bot_pro_forecasts = pd.read_csv('your_data.csv')\n",
    "\n",
    "def create_discrimination_histogram(df, bot_col, pro_col, resolution_col):\n",
    "    # Create figure and axes\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "    # Define bin edges\n",
    "    #bins = np.linspace(0, 1, 11)\n",
    "    bins = np.linspace(0, 1, 6)\n",
    "\n",
    "    # Bot team histogram\n",
    "    ax1.hist([df[df[resolution_col] == 0][bot_col],\n",
    "              df[df[resolution_col] == 1][bot_col]],\n",
    "             bins=bins, label=['Resolved 0', 'Resolved 1'], alpha=0.7)\n",
    "    ax1.set_title('Bot Team Discrimination Histogram')\n",
    "    ax1.set_xlabel('Probability')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Pro team histogram\n",
    "    ax2.hist([df[df[resolution_col] == 0][pro_col],\n",
    "              df[df[resolution_col] == 1][pro_col]],\n",
    "             bins=bins, label=['Resolved 0', 'Resolved 1'], alpha=0.7)\n",
    "    ax2.set_title('Pro Team Discrimination Histogram')\n",
    "    ax2.set_xlabel('Probability')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your DataFrame and column names\n",
    "create_discrimination_histogram(df_top_bot_pro_forecasts,\n",
    "                                'bot_team_median',\n",
    "                                'pro_median',\n",
    "                                'resolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dkNBotk_4e3",
    "outputId": "d393a72e-997a-4025-ca7b-6f5328436286"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate average forecasts for resolved 1 and 0 for bots\n",
    "bot_avg_1 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 1]['bot_team_median'].mean()\n",
    "bot_avg_0 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 0]['bot_team_median'].mean()\n",
    "\n",
    "# Calculate average forecasts for resolved 1 and 0 for pros\n",
    "pro_avg_1 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 1]['pro_median'].mean()\n",
    "pro_avg_0 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 0]['pro_median'].mean()\n",
    "\n",
    "# Calculate the differences\n",
    "bot_difference = bot_avg_1 - bot_avg_0\n",
    "pro_difference = pro_avg_1 - pro_avg_0\n",
    "\n",
    "print(f\"Bot average forecast difference (1 - 0): {bot_difference:.4f}\")\n",
    "print(f\"Pro average forecast difference (1 - 0): {pro_difference:.4f}\")\n",
    "\n",
    "# Calculate the difference between pro and bot differences\n",
    "pro_bot_difference = pro_difference - bot_difference\n",
    "print(f\"Difference between pro and bot differences: {pro_bot_difference:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgbKopkoEyKZ",
    "outputId": "f4c39cfc-2626-47fa-f4fd-e68c816087df"
   },
   "outputs": [],
   "source": [
    "# TK: Ask Tom for scope sensitivity questions?\n",
    "\n",
    "# Assuming df_top_bot_pro_forecasts is already loaded\n",
    "\n",
    "if False:\n",
    "\n",
    "    def get_forecast(question_id):\n",
    "        forecast = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['bot_question_id'] == question_id]['bot_team_median'].values\n",
    "        return forecast[0] if len(forecast) > 0 else np.nan\n",
    "\n",
    "    def process_scope_questions(bot_scope_questions):\n",
    "        scope_ratios = []\n",
    "\n",
    "        for question_tuple in bot_scope_questions:\n",
    "            forecasts = [get_forecast(qid) for qid in question_tuple]\n",
    "\n",
    "            print(f\"Forecasts for questions {question_tuple}: {forecasts}\")\n",
    "\n",
    "            if not any(np.isnan(forecasts)):\n",
    "                scope_ratio = forecasts[0] / (forecasts[1] + forecasts[2])\n",
    "                print(f\"Scope ratio: {scope_ratio:.4f}\")\n",
    "                scope_ratios.append(scope_ratio)\n",
    "            else:\n",
    "                print(\"Unable to calculate scope ratio due to missing forecast(s)\")\n",
    "\n",
    "            print()  # Empty line for readability\n",
    "\n",
    "        return scope_ratios\n",
    "\n",
    "    # Process the bot_scope_questions\n",
    "    scope_ratios = process_scope_questions(bot_scope_questions)\n",
    "\n",
    "    # Calculate and print the average scope ratio\n",
    "    if scope_ratios:\n",
    "        average_scope_ratio = sum(scope_ratios) / len(scope_ratios)\n",
    "        print(f\"Average scope ratio: {average_scope_ratio:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid scope ratios to average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGnXswWOx_yw",
    "outputId": "35a0e2a8-5831-43cf-a006-f8e0262666ec"
   },
   "outputs": [],
   "source": [
    "# Calculate weighted number of 1 resolutions\n",
    "weighted_ones = np.sum(\n",
    "    df_top_bot_pro_forecasts['resolution'] *\n",
    "    df_top_bot_pro_forecasts['question_weight']\n",
    ")\n",
    "\n",
    "# Calculate weighted number of 0 resolutions\n",
    "weighted_zeros = np.sum(\n",
    "    (1 - df_top_bot_pro_forecasts['resolution']) *\n",
    "    df_top_bot_pro_forecasts['question_weight']\n",
    ")\n",
    "\n",
    "print(f\"Weighted number of 1 resolutions: {weighted_ones}\")\n",
    "print(f\"Weighted number of 0 resolutions: {weighted_zeros}\")\n",
    "\n",
    "print(f\"Average 1 resolutions: {weighted_ones / (weighted_zeros + weighted_ones)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CP COMPARISON (LUKE: LOOK HERE DOWN)\n",
    "\n",
    "cp = pd.read_csv('https://data.heroku.com/dataclips/xwbtczmsuszvlbrhdifhsilplfxf.csv')\n",
    "cp.rename(columns={'post_id': 'cp_post_id', 'question_id': 'cp_question_id'}, inplace=True)\n",
    "\n",
    "bot_cp_id = pd.read_csv('bot_to_main_feed_ids.csv')\n",
    "                     \n",
    "# Merge these on cp_question_id\n",
    "df_bot_cp = pd.merge(bot_cp_id, cp, on='cp_post_id', how='right')\n",
    "\n",
    "df_bot_cp = df_bot_cp[df_bot_cp['bot_question_id'].notnull()]\n",
    "df_bot_cp['bot_question_id'] = df_bot_cp['bot_question_id'].astype(int)\n",
    "\n",
    "# Evaluate cp_reveal_time, start_time, and end_time as datetime objects\n",
    "df_bot_cp['cp_reveal_time'] = pd.to_datetime(df_bot_cp['cp_reveal_time'])\n",
    "df_bot_cp['start_time'] = pd.to_datetime(df_bot_cp['start_time'])\n",
    "df_bot_cp['end_time'] = pd.to_datetime(df_bot_cp['end_time'])\n",
    "\n",
    "# For each group of (bot_question_id, question_title, cp_reveal_time), take only the row with the start_time closest to (BUT LESS THAN) cp_reveal_time\n",
    "df_bot_cp = df_bot_cp.sort_values(by=['bot_question_id', 'cp_reveal_time', 'start_time'])\n",
    "df_bot_cp = df_bot_cp[df_bot_cp['start_time'] < df_bot_cp['cp_reveal_time']]\n",
    "df_bot_cp = df_bot_cp.drop_duplicates(subset=['bot_question_id', 'cp_reveal_time'], keep='last')\n",
    "\n",
    "# Convert string representation of lists to actual lists\n",
    "df_bot_cp['forecast_values'] = df_bot_cp['forecast_values'].str.strip('[]').str.split(',').apply(lambda x: [float(i.strip()) for i in x])\n",
    "\n",
    "# How many rows?\n",
    "print(f\"Number of rows: {len(df_bot_cp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down by types - \"group\" (multiple choice; my bad), \"binary\" and \"numeric\"\n",
    "\n",
    "# Group questions are the ones that have NON-EMPTY lists in the options column\n",
    "groups = df_bot_cp[df_bot_cp['type'] == 'multiple_choice']\n",
    "groups['options'] = groups['options'].str.strip('[]').str.split(',').apply(lambda x: [i.strip().strip(\"'\") for i in x])\n",
    "\n",
    "binaries = df_bot_cp[df_bot_cp['type'] == 'binary']\n",
    "\n",
    "numerics = df_bot_cp[df_bot_cp['type'] == 'numeric']\n",
    "\n",
    "keep_cols = ['bot_question_id', 'question_title', 'title', 'cp_reveal_time', 'type', 'cp_question_id', 'cp_post_id', 'forecast_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and store problematic index\n",
    "problematic_idx = None\n",
    "for idx, row in groups.iterrows():\n",
    "   if len(row['forecast_values']) != len(row['options']):\n",
    "       problematic_idx = idx\n",
    "       break\n",
    "\n",
    "# Fix the specific row using stored index\n",
    "if problematic_idx is not None:\n",
    "   groups.at[problematic_idx, 'options'] = [\n",
    "       'Low',\n",
    "       'Moderate (or medium or equivalent)',\n",
    "       'High (or above such as Very High)'\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_exploded = groups.explode(['options', 'forecast_values'])\n",
    "groups_exploded['options'] = groups_exploded['options'].str.strip('\"')\n",
    "\n",
    "groups_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for matching rows\n",
    "mask = groups_exploded.apply(lambda row: row['options'] in row['question_title'], axis=1)\n",
    "\n",
    "# Split into matching and non-matching\n",
    "matched_df = groups_exploded[mask]\n",
    "unmatched_df = groups_exploded[~mask]\n",
    "\n",
    "print(f\"Matched rows: {len(matched_df)}\")\n",
    "print(f\"Unmatched rows: {len(unmatched_df)}\")\n",
    "print(\"\\nSample unmatched rows:\")\n",
    "unmatched_df[['question_title', 'options']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joe Biden questions are tricky to match - TK: Deal with These Later\n",
    "\n",
    "#groups_exploded[groups_exploded['question_title'].str.contains('Joe Biden')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BINARIES: Interpret forecast_values as lists and take the first element from each\n",
    "binaries['forecast_values'] = binaries['forecast_values'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics[['bot_question_id', 'question_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICS ARE TRICKY\n",
    "\n",
    "# How long is each list in forecast_values?\n",
    "numerics['cdf_size'] = numerics['forecast_values'].apply(len)\n",
    "\n",
    "# Forbes Real-Time Billionaires highest other net worth (for Elon Musk question)\n",
    "bezos = 240 # TK: Check what value was actually used for resolution\n",
    "\n",
    "# Need to extract thresholds from binary versions of the numeric questions. TK: Could use another pair of eyes\n",
    "thresholds = {\n",
    "   29163: ('less', 2.0),        # COVID hospitalizations\n",
    "   29349: ('greater', 100),     # Brasilia rain\n",
    "   29350: ('greater', 150),     # Brasilia rain \n",
    "   29351: ('greater', 200),     # Brasilia rain\n",
    "   29353: ('greater', 20),      # Arms sales\n",
    "   29354: ('greater', 25),      # Arms sales\n",
    "   29362: ('greater', 3900),    # Emojis\n",
    "   29461: ('greater', 2000),    # Influenza hospitalizations\n",
    "   29462: ('greater', 2000),    # Influenza hospitalizations\n",
    "   29463: ('greater', 80),      # CDC influenza A\n",
    "   29566: ('less', 17.0),       # China unemployment Oct\n",
    "   29567: ('complicated', 0.0), # China unemployment Oct\n",
    "   29568: ('complicated', 0.0), # China unemployment Oct\n",
    "   29569: ('greater', 19.0),    # China unemployment Oct\n",
    "   29642: ('less', 240),        # Elon Musk net worth\n",
    "   29643: ('complicated', 0.0), # Elon Musk net worth\n",
    "   29644: ('complicated', 0.0), # Elon Musk net worth\n",
    "   29645: ('complicated', 0.0), # Elon Musk net worth\n",
    "   29646: ('greater', 340),     # Elon Musk net worth\n",
    "   29836: ('less', 17.0),       # China unemployment Nov\n",
    "   29837: ('complicated', 0.0), # China unemployment Nov\n",
    "   29838: ('complicated', 0.0), # China unemployment Nov\n",
    "   29839: ('greater', 19.0),    # China unemployment Nov\n",
    "   29836: ('greater', 375),     # NZ whooping cough\n",
    "   30578: ('complicated', 0.0),     # NZ whooping cough\n",
    "   30579: ('less', 275),         # NZ whooping cough\n",
    "   30440: ('greater', -4),      # Trump favorability\n",
    "   30441: ('complicated', 0.0),      # Trump favorability\n",
    "   30442: ('less', -6),         # Trump favorability\n",
    "   30583: ('greater', 7400),    # CAC 40\n",
    "   30584: ('complicated', 0.0),       # CAC 40\n",
    "   30585: ('less', 7200)        # CAC 40\n",
    "}\n",
    "\n",
    "# Apply that dictionary and make a 'binary_version_tuple' column\n",
    "numerics['binary_version_tuple'] = numerics['bot_question_id'].map(thresholds)\n",
    "\n",
    "numerics.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics[['binary_version_tuple', 'forecast_values']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM LUKE, REFACTORED BY CHATGPT\n",
    "\n",
    "def string_location_to_scaled_location(string_location: str, question_row: pd.Series) -> float:\n",
    "    if string_location in [\"ambiguous\", \"annulled\"]:\n",
    "        raise ValueError(\"Cannot convert ambiguous or annulled to any real locations\")\n",
    "\n",
    "    question_type = question_row[\"type\"]\n",
    "\n",
    "    if question_type == \"binary\":\n",
    "        return 1.0 if string_location == \"yes\" else 0.0\n",
    "\n",
    "    if question_type == \"multiple_choice\":\n",
    "        return float(question_row[\"options\"].index(string_location))\n",
    "\n",
    "    # continuous\n",
    "    if string_location == \"below_lower_bound\":\n",
    "        return question_row[\"range_min\"] - 1.0\n",
    "    if string_location == \"above_upper_bound\":\n",
    "        return question_row[\"range_max\"] + 1.0\n",
    "\n",
    "    if question_type == \"date\":\n",
    "        return datetime.fromisoformat(string_location).timestamp()\n",
    "\n",
    "    # question.type == \"numeric\"\n",
    "    return float(string_location)\n",
    "\n",
    "def scaled_location_to_unscaled_location(scaled_location: float, question_row: pd.Series) -> float:\n",
    "    question_type = question_row[\"type\"]\n",
    "\n",
    "    if question_type in [\"binary\", \"multiple_choice\"]:\n",
    "        return scaled_location\n",
    "\n",
    "    zero_point = question_row.get(\"zero_point\")\n",
    "    range_max = question_row[\"range_max\"]\n",
    "    range_min = question_row[\"range_min\"]\n",
    "\n",
    "    if zero_point is not None:\n",
    "        deriv_ratio = (range_max - zero_point) / max((range_min - zero_point), 1e-7)\n",
    "        return (\n",
    "            np.log((scaled_location - range_min) * (deriv_ratio - 1) + (range_max - range_min))\n",
    "            - np.log(range_max - range_min)\n",
    "        ) / np.log(deriv_ratio)\n",
    "\n",
    "    return (scaled_location - range_min) / (range_max - range_min)\n",
    "\n",
    "def nominal_location_to_cdf_location(\n",
    "    nominal_location: str | float,\n",
    "    question_data: dict,\n",
    ") -> float:\n",
    "    \"\"\"Takes a location in nominal format (e.g. 123, \"123\",\n",
    "    or datetime in iso format) and scales it to metaculus's\n",
    "    \"internal representation\" range [0,1] incorporating question scaling\"\"\"\n",
    "    if question_data[\"type\"] == \"date\":\n",
    "        scaled_location = datetime.fromisoformat(nominal_location).timestamp()\n",
    "    else:\n",
    "        scaled_location = float(nominal_location)\n",
    "    # Unscale the value to put it into the range [0,1]\n",
    "    range_min = question_data[\"range_min\"]\n",
    "    range_max = question_data[\"range_max\"]\n",
    "    zero_point = question_data[\"zero_point\"]\n",
    "    if ~np.isnan(zero_point) and (zero_point is not None):\n",
    "        # logarithmically scaled question\n",
    "        deriv_ratio = (range_max - zero_point) / (range_min - zero_point)\n",
    "        unscaled_location = (\n",
    "            np.log(\n",
    "                (scaled_location - range_min) * (deriv_ratio - 1)\n",
    "                + (range_max - range_min)\n",
    "            )\n",
    "            - np.log(range_max - range_min)\n",
    "        ) / np.log(deriv_ratio)\n",
    "    else:\n",
    "        # linearly scaled question\n",
    "        unscaled_location = (scaled_location - range_min) / (range_max - range_min)\n",
    "    return unscaled_location\n",
    "\n",
    "def get_cdf_at(cdf, unscaled_location) -> float:\n",
    "    \"\"\"CDF is a list of values, unscaled_location is a float\n",
    "    with 0 meaning lower bound and 1 meaning upper bound\"\"\"\n",
    "    if unscaled_location <= 0:\n",
    "        return cdf[0]\n",
    "    if unscaled_location >= 1:\n",
    "        return cdf[-1]\n",
    "    index_scaled_location = unscaled_location * (len(cdf) - 1)\n",
    "    if index_scaled_location.is_integer():\n",
    "        return cdf[int(index_scaled_location)]\n",
    "    # linear interpolation step\n",
    "    left_index = int(index_scaled_location) # This is the floor, which is what we want\n",
    "    right_index = left_index + 1\n",
    "    left_value = cdf[left_index]\n",
    "    right_value = cdf[right_index]\n",
    "    return left_value + (right_value - left_value) * (\n",
    "        index_scaled_location - left_index\n",
    "    )\n",
    "\n",
    "#def unscaled_location_to_bucket_index(unscaled_location: float, question_row: pd.Series) -> int:\n",
    "    #question_type = question_row[\"type\"]\n",
    "\n",
    "    #if question_type in [\"binary\", \"multiple_choice\"]:\n",
    "        #return int(unscaled_location)\n",
    "\n",
    "    ## continuous\n",
    "    #if unscaled_location < 0:\n",
    "        #return 0\n",
    "    #if unscaled_location > 1:\n",
    "        #return question_row[\"cdf_size\"]\n",
    "    #if unscaled_location == 1:\n",
    "        #return question_row[\"cdf_size\"] - 1\n",
    "\n",
    "    #return max(int(unscaled_location * (question_row[\"cdf_size\"] - 1) + 1 - 1e-10), 1)\n",
    "\n",
    "#def string_location_to_unscaled_location(string_location: str, question_row: pd.Series) -> float:\n",
    "    #if string_location in [None, \"ambiguous\", \"annulled\"]:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #return None\n",
    "\n",
    "    #scaled_location = string_location_to_scaled_location(string_location, question_row)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #return scaled_location_to_unscaled_location(scaled_location, question_row)\n",
    "\n",
    "#def string_location_to_bucket_index(string_location: str, question_row: pd.Series) -> int | None:\n",
    "    #if string_location in [None, \"ambiguous\", \"annulled\"]:\n",
    "        #return None\n",
    "\n",
    "    #unscaled_location = string_location_to_unscaled_location(string_location, question_row)\n",
    "    #return unscaled_location_to_bucket_index(unscaled_location, question_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values of binary_version_tuple\n",
    "unique_tuples = numerics['binary_version_tuple'].unique()\n",
    "unique_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cdf's for the complicated ones (we will overwrite forecast_values)\n",
    "numerics['cdf'] = numerics['forecast_values']\n",
    "\n",
    "def process_forecast_values(df):\n",
    "    \"\"\"\n",
    "    Adds a new column 'bucket_forecast_value' to the DataFrame.\n",
    "    Handles 'binary_version_tuple' and applies logic for 'less', 'greater', and 'complicated'.\n",
    "\n",
    "    :param df: DataFrame with 'binary_version_tuple', 'forecast_values', and other question-specific columns\n",
    "    :return: Updated DataFrame with 'bucket_forecast_value' column added\n",
    "    \"\"\"\n",
    "    def compute_bucket_forecast_value(row):\n",
    "        # Handle binary_version_tuple gracefully\n",
    "        if pd.isna(row['binary_version_tuple']) or not isinstance(row['binary_version_tuple'], (list, tuple)):\n",
    "            return None\n",
    "        \n",
    "        # Extract the first and second elements of the tuple\n",
    "        comparison_type = row['binary_version_tuple'][0]\n",
    "        string_location = row['binary_version_tuple'][1]\n",
    "        \n",
    "        # Skip if comparison_type is 'complicated'\n",
    "        if comparison_type == 'complicated':\n",
    "            return None\n",
    "        \n",
    "        # Compute forecast_value using the extracted string_location\n",
    "        forecast_value = get_cdf_at(row['cdf'], nominal_location_to_cdf_location(string_location, row))\n",
    "        \n",
    "        # Apply logic based on comparison_type\n",
    "        if comparison_type == 'less':\n",
    "            return forecast_value\n",
    "        elif comparison_type == 'greater':\n",
    "            return 1 - forecast_value\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Apply the function to each row and overwrite forecast_value (currently contains cdf, which we no longer need)\n",
    "    df['forecast_values'] = df.apply(compute_bucket_forecast_value, axis=1)\n",
    "    return df\n",
    "\n",
    "numerics = process_forecast_values(numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show me the rows where forecast_values is NaN\n",
    "numerics[numerics['forecast_values'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doing the \"between\" numerics one by one by bot_question_id\n",
    "\n",
    "def cdf_between(row, cdf, lower_bound, upper_bound):\n",
    "  a = get_cdf_at(cdf, nominal_location_to_cdf_location(lower_bound, row))\n",
    "  b = get_cdf_at(cdf, nominal_location_to_cdf_location(upper_bound, row))\n",
    "  numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = b - a\n",
    "\n",
    "# 29503: Waymo exactly 4, i.e. between 3.5 and 4.5 on continuous question\n",
    "row = numerics[numerics['bot_question_id'] == 29503].iloc[0]\n",
    "cdf_between(row, row['cdf'], 3.5, 4.5)\n",
    "\n",
    "# 29567: China youth unemployment > 17.0 and less than 18.0\n",
    "row = numerics[numerics['bot_question_id'] == 29567].iloc[0]\n",
    "cdf_between(row, row['cdf'], 17.0, 18.0)\n",
    "\n",
    "# 29568: China youth unemployment > 18.0 and less than 19.0\n",
    "row = numerics[numerics['bot_question_id'] == 29568].iloc[0]\n",
    "cdf_between(row, row['cdf'], 18.0, 19.0)\n",
    "\n",
    "# 29643: Elon Musk net worth > 240 and less than 280\n",
    "row = numerics[numerics['bot_question_id'] == 29643].iloc[0]\n",
    "cdf_between(row, row['cdf'], 240, 280)\n",
    "\n",
    "# 29644: Elon Musk net worth > 280 and less than 310\n",
    "row = numerics[numerics['bot_question_id'] == 29644].iloc[0]\n",
    "cdf_between(row, row['cdf'], 280, 310)\n",
    "\n",
    "# 29645: Elon Musk net worth > 310 and less than 340\n",
    "row = numerics[numerics['bot_question_id'] == 29645].iloc[0]\n",
    "cdf_between(row, row['cdf'], 310, 340)\n",
    "\n",
    "# 29837: China youth unemployment > 17.0 and less than 18.0\n",
    "row = numerics[numerics['bot_question_id'] == 29837].iloc[0]\n",
    "cdf_between(row, row['cdf'], 17.0, 18.0)\n",
    "\n",
    "# 29838: China youth unemployment > 18.0 and less than 19.0\n",
    "row = numerics[numerics['bot_question_id'] == 29838].iloc[0]\n",
    "cdf_between(row, row['cdf'], 18.0, 19.0)\n",
    "\n",
    "# 30281: Waymo exactly 4, i.e. between 3.5 and 4.5 on continuous question\n",
    "row = numerics[numerics['bot_question_id'] == 30281].iloc[0]\n",
    "cdf_between(row, row['cdf'], 3.5, 4.5)\n",
    "\n",
    "# 30437: New Zealand >375 whooping cough cases\n",
    "row = numerics[numerics['bot_question_id'] == 30437].iloc[0]\n",
    "cdf_between(row, row['cdf'], 375, 400)\n",
    "\n",
    "# 30438: New Zealand >275 and less than 375 whooping cough cases\n",
    "row = numerics[numerics['bot_question_id'] == 30438].iloc[0]\n",
    "cdf_between(row, row['cdf'], 275, 375)\n",
    "\n",
    "# 30439: New Zealand less than 275 whooping cough cases\n",
    "row = numerics[numerics['bot_question_id'] == 30439].iloc[0]\n",
    "cdf_between(row, row['cdf'], 250, 275)\n",
    "\n",
    "# 30441: Trump net favorabilty > -6 and less than -4\n",
    "row = numerics[numerics['bot_question_id'] == 30441].iloc[0]\n",
    "cdf_between(row, row['cdf'], -6, -4)\n",
    "\n",
    "# 30584: CAC 40 > 7200 and less than 7400\n",
    "row = numerics[numerics['bot_question_id'] == 30584].iloc[0]\n",
    "cdf_between(row, row['cdf'], 7200, 7400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = numerics[keep_cols]\n",
    "binaries = binaries[keep_cols]\n",
    "groups_exploded = groups_exploded[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can merge all back together into.... df_bot_cp_exploded; keep only the relevant columns, i.e. 'bot_question_id', 'cp_question_id', 'cp_post_id', 'forecast_values'\n",
    "df_bot_cp_exploded = pd.concat([groups_exploded, binaries, numerics])\n",
    "df_bot_cp_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show me rows that are 0 in forecast_values\n",
    "df_bot_cp_exploded[df_bot_cp_exploded['forecast_values'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gracefully compute cp_baseline_score\n",
    "def compute_cp_baseline_score(value):\n",
    "    try:\n",
    "        # Ensure the value is numeric and not NaN\n",
    "        if pd.isna(value) or not isinstance(value, (int, float)):\n",
    "            return np.nan\n",
    "        # Perform the calculation\n",
    "        return 100 * np.log(value - np.log(0.5)) / np.log(2)\n",
    "    except Exception:\n",
    "        # Handle any unexpected errors\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to compute cp_baseline_score\n",
    "df_bot_cp_exploded['cp_baseline_score'] = df_bot_cp_exploded['forecast_values'].apply(compute_cp_baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bot_cp_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows are NaN in cp_baseline_score?\n",
    "print(f'{df_bot_cp_exploded['cp_baseline_score'].isna().sum()} out of {len(df_bot_cp_exploded)} rows are NaN in cp_baseline_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_forecasts.merge(df_bot_cp_exploded[['bot_question_id', 'cp_post_id', 'forecast_values']], on='bot_question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many questions don't have a CP forecast?\n",
    "print(f'{df_top_bot_pro_cp_forecasts['forecast_values'].isna().sum()} out of {len(df_top_bot_pro_cp_forecasts)} rows are NaN in forecast_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN in forecast_values\n",
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_cp_forecasts.dropna(subset=['forecast_values'])\n",
    "# Cast forecast_values as float\n",
    "df_top_bot_pro_cp_forecasts['forecast_values'] = df_top_bot_pro_cp_forecasts['forecast_values'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_bot_pro_cp_forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TK: Make this nice calibration curve with error bars\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "\n",
    "# Plot calibration curves for bot_team_median and pro_median\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'bot_team_median', 'Bot Team Median', 'blue')\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'pro_median', 'Pro Median', 'red')\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'forecast_values', 'Community Prediction', 'green')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Predicted probability', fontsize=12)\n",
    "plt.ylabel('Actual Outcome Probability', fontsize=12)\n",
    "plt.title('Calibration Curve: Bot Team Median vs Pro Median vs Community Prediction', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add diagonal line for perfect calibration\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
