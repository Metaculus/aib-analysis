{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP64XWyvNkZl"
   },
   "source": [
    "Main Leaderboard\n",
    "\n",
    "Forecaster  | Peer score against Pros | % Beat Pros on same Qs (Bootstrap) | % Beat Pros on same Qs (Resample)\n",
    "\n",
    "Resample: Linear interpolation of two forecasters to determine \"ground truth\" for resampling. \"Ground truth\" = weighted combination of percent forecasts on each question that yields the most accurate score. Weights are 0 to 1.\n",
    "\n",
    "Correlation: Deal with by assigning weights to questions. Weights for independent questions are 1. \"Approximately correct rather than precisely wrong.\" Would be good to have a rule of thumb. These weights can be used for leaderboard above.\n",
    "\n",
    "Likely want % beat Pros to be beyond 95% for significance - but obviously a sliding scale.\n",
    "\n",
    "Can use this method for Metaculus Track Record! Have separate line for each platform - so can see significance against each platform. Then combine all competeing platforms and treat as the same forecaster going head-to-head against Metaculus!\n",
    "\n",
    "Heroku:\n",
    "\n",
    "Bots: https://data.heroku.com/dataclips/bmlboxtaewpwemfvqwktqxernfeq\n",
    "\n",
    "Pros: https://data.heroku.com/dataclips/rozqhydlvqrzsllgmioruallozjx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISzIoto4hnoG"
   },
   "outputs": [],
   "source": [
    "# @title Import libraries\n",
    "from functions import *\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_head_and_tail(df: pd.DataFrame):\n",
    "  display(df.head())\n",
    "  display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create df_bot_resolved_questions, df_pro_resolved_questions, df_pro_bot_resolved_questions, df_bot_question_weights\n",
    "\n",
    "\"\"\"\n",
    "Input question data for both bots and pros.\n",
    "\n",
    "Only look at questions that have resolved Yes or No.\n",
    "\n",
    "df_pro_resolved_questions: Has pro_question_id, title, resolution, scheduled_close_time\n",
    "df_bot_resolved_questions: Has bot_question_id, title, resolution, scheduled_close_time\n",
    "\n",
    "All pro questions are asked to bots, but not all bot questions are asked to pros (correction:\n",
    "not true in 2024 Q4, there were some that got launched to pros first? and were bad so they\n",
    "didn't get asked of bots?)\n",
    "\n",
    "To compare pros to bots, we need to match the pro_question_id with the bot_question_id.\n",
    "This is done by matching the title and scheduled_close_time.\n",
    "\n",
    "We remove early closers from the analysis. I do this by comparing actual close time to scheduled\n",
    "close time in a later cell!\n",
    "\n",
    "df_pro_bot_resolved_questions: Has pro_question_id, bot_question_id, title, resolution, scheduled_close_time, question_weight\n",
    "\"\"\"\n",
    "\n",
    "# Create dictionary with keys \"Q1\" \"Q4\" and the things are links\n",
    "q_data = {\n",
    "    'q1': {\n",
    "        'bots': 'https://data.heroku.com/dataclips/nqghgczhvwahbmupzypyzaanabzv.csv',\n",
    "        'pros': 'https://data.heroku.com/dataclips/pcyecxbmoxppkxxaebikcwukhpqk.csv'\n",
    "    },\n",
    "    'q4': {\n",
    "        'bots': 'https://data.heroku.com/dataclips/nudnpycciffydoeihwbtttlkwpcj.csv',\n",
    "        'pros': 'https://data.heroku.com/dataclips/dgoglqeavaxrhhnfcikkoobuollk.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "quarter = 'q1'\n",
    "\n",
    "df_bot_scores = pd.read_csv(q_data[quarter]['bots'])\n",
    "df_bot_scores.to_csv(f'scores/bots_score_data_{quarter}.csv', index=False)\n",
    "#df_bot_scores = pd.read_csv('scores/bots_score_data_q3.csv')\n",
    "df_bot_questions = df_bot_scores.rename(columns={'question_id': 'bot_question_id', 'question_title': 'title'})\n",
    "df_bot_questions = df_bot_questions[df_bot_questions['resolution'].isin(['yes', 'no'])]\n",
    "\n",
    "df_pro_scores = pd.read_csv(q_data[quarter]['pros'])\n",
    "df_pro_scores.to_csv(f'scores/pros_score_data_{quarter}.csv', index=False)\n",
    "#df_pro_scores = pd.read_csv('scores/pros_score_data_q3.csv')\n",
    "df_pro_questions = df_pro_scores.rename(columns={'question_id': 'pro_question_id', 'question_title': 'title'})\n",
    "df_pro_questions = df_pro_questions[df_pro_questions['resolution'].isin(['yes', 'no'])]\n",
    "\n",
    "df_pro_resolved_questions = df_pro_questions[['pro_question_id', 'title', 'resolution', 'scheduled_close_time', 'actual_close_time', 'question_weight']]\n",
    "df_bot_resolved_questions = df_bot_questions[['bot_question_id', 'title', 'resolution', 'scheduled_close_time', 'actual_close_time', 'question_weight']]\n",
    "\n",
    "df_pro_bot_resolved_questions = pd.merge(\n",
    "    df_bot_resolved_questions,\n",
    "    df_pro_resolved_questions[['pro_question_id', 'title', 'scheduled_close_time', 'question_weight']],\n",
    "    on=['title', 'scheduled_close_time'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_resolved_questions['question_weight'] = df_pro_bot_resolved_questions['question_weight_x'].combine_first(df_pro_bot_resolved_questions['question_weight_y'])\n",
    "df_pro_bot_resolved_questions.drop(['question_weight_x', 'question_weight_y'], axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df_pro_bot_resolved_questions = df_pro_bot_resolved_questions.drop_duplicates()\n",
    "\n",
    "# Cast both question ids to int64\n",
    "df_pro_bot_resolved_questions['pro_question_id'] = df_pro_bot_resolved_questions['pro_question_id'].astype('Int64')\n",
    "df_pro_bot_resolved_questions['bot_question_id'] = df_pro_bot_resolved_questions['bot_question_id'].astype('Int64')\n",
    "\n",
    "# Remove df_bot_resolved_questions and df_pro_resolved_questions to make sure you only ever use df_pro_bot_resolved_questions\n",
    "del df_bot_resolved_questions\n",
    "del df_pro_resolved_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are any of the weights NOT 1 -- for Q3 we need to assign weights \"manually\" but for Q4 they are there\n",
    "print(df_pro_bot_resolved_questions[df_pro_bot_resolved_questions['question_weight'] != 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df_pro_bot_resolved_questions.shape\n",
    "\n",
    "# How many are NA for pro_question_id?\n",
    "a = df_pro_bot_resolved_questions['pro_question_id'].isna().sum()\n",
    "\n",
    "print(f'Total number bot questions: {b[0]}')\n",
    "print(f'Bot questions that don\\'t have pro counterpart: {a}')\n",
    "print(f'Overlap between bot and pro questions: {b[0]-a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted vs unweighted breakdown for those overlapping questions?\n",
    "df_pro_bot_overlap = df_pro_bot_resolved_questions[~df_pro_bot_resolved_questions['pro_question_id'].isna()]\n",
    "print(f'Unweighted count: {df_pro_bot_overlap.shape[0]}')\n",
    "print(f'Weighted count: {df_pro_bot_overlap['question_weight'].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Relationships between Bot Questions, create df_bot_question_related_weights (FOR Q3 ONLY)\n",
    "if 25871 in df_pro_bot_resolved_questions['bot_question_id'].values:\n",
    "  \"\"\"\n",
    "  Relationships between questions are entered as tuples. These relationships\n",
    "  will be used to perform logical consistency checks.\n",
    "\n",
    "  Weights are assigned to questions based on relationships. This is a way to\n",
    "  deal with correlations between questions.\n",
    "  \"\"\"\n",
    "\n",
    "  # Scope sensitity list of tuples where the first entry should equal the sum of the others\n",
    "  bot_scope_questions = [\n",
    "        (26019, 26017, 26018), # Starship launches\n",
    "        (26098, 26096, 26097), # SENSEX\n",
    "        (26159, 26158, 26157), # Geomagnetic storm July 28\n",
    "        (26194, 26195, 26196), # measles cases\n",
    "        (26006, 26005, 26004), # Trump lead over Biden\n",
    "        (26642, 26643, 26644), # spanish wikipedia\n",
    "        (26700, 26701, 26702), # market cap cryptocurrencies\n",
    "        (27261, 27262, 27263), # Geomagnetic storm Sept 11\n",
    "        ]\n",
    "\n",
    "  # Sum of each tuple should logically equal 1\n",
    "  bot_sum_to_1_questions = [\n",
    "      (25952, 25953, 25954), # French PM party July 30\n",
    "      (25957, 25958, 25959), # Tour de France winner\n",
    "      (26570, 26571, 26572, 26573), # Warhammer\n",
    "      (26574, 26575, 26576, 26577), # H5 cases in US\n",
    "      (26671, 26670, 26669), # DOES NOT SUM TO EXACTLY 1 PM France Aug 31\n",
    "      (27748, 27747, 27746, 27749), # Speed Chess\n",
    "      (27488, 27489, 27490, 27491, 27492, 27493), # August CPI\n",
    "      (27932, 27933, 27934, 27935), # Chinese youth unemployment\n",
    "      (27484, 27485, 27486, 27487), # Fed rate cut Sept meeting\n",
    "      (28045, 28044, 28043, 28042), # Afd vote share\n",
    "      (28038, 28039, 28040, 28041), # Major Atlantic hurricanes\n",
    "      (26776, 26777, 26778, 26779), # Seattle-Tacoma-Bellevu Air Quality\n",
    "      ]\n",
    "\n",
    "  # parent, child, if_yes, if_no\n",
    "  bot_conditional_pair = [\n",
    "      (26917, 26918, 26919, 26920) # israel lebanon conflict\n",
    "  ]\n",
    "\n",
    "  # CDFs - Logically the probability of each successive question must not decrease\n",
    "  bot_increasing_questions = [\n",
    "      (26981, 26982, 26983, 26984, 26985, 26986), # aircraft ADIZ\n",
    "      (26977, 26978, 26979, 26980), # hurricane energy\n",
    "      (27548, 27547, 27546, 27545), # mpox CDC risk level\n",
    "      (28306, 28305, 28304, 28303, 28302), # Gas prices in US Sept 30\n",
    "  ]\n",
    "\n",
    "  bot_repeated_questions = [\n",
    "      (26646, 26021), # mens 100m dash record\n",
    "      (26555, 27021), # USA gold silver\n",
    "      (26210, 26917), # israel invade lebanon\n",
    "      (26781, 26304), # ruto\n",
    "      (26100, 27136), # rfk drop out\n",
    "      (25956, 27158), # democrat brokered convention\n",
    "      (26102, 27022), # astronauts NOT EXACT REPEAT\n",
    "      (26022, 27085), # arrest warrants NOT EXACT REPEAT\n",
    "      (26235, 27281), # Buffett Indicator\n",
    "      (26390, 27789), # Bubble Magnificent 7\n",
    "      (26024, 27161), # QB Bo Nix starting for Broncos\n",
    "      (26302, 27282), # riots\n",
    "      (25955, 27157), # armed forces death US, China, Japan\n",
    "      (26958, 27640), # Youtube banned in Russia\n",
    "      (25936, 27141), # Crimean bridge attack\n",
    "  ]\n",
    "\n",
    "  bot_similar_questions = [\n",
    "      (26915, 26916), # harris favorability\n",
    "      (26913, 26914), # trump favorability\n",
    "      (26193, 27733), # debate on Sept 10\n",
    "      (27886, 27968), # Taylor Swift awards\n",
    "      (27723, 27637), # Best Rock VMAs\n",
    "      (27583, 27582, 27584, 27602, 27603, 27604), # mpox Zambia, US, Angola, Russia, Japan, Mexico\n",
    "      (26306, 26838), # Richest people 250th > $10.2, 500th > 6.2\n",
    "      (27887, 27969), # Emmys Outstanding Limited or Anthology Series\n",
    "      (28206, 28207, 28208, 28209, 28210), # LMSYS leaderboard\n",
    "      (28154, 28336), # Nigeria Edo gubernatorial election\n",
    "      (26407, 27897), # Second Russian mobilization wave\n",
    "      (27539, 26215), # Nuclear weapons used\n",
    "      (27606, 27607, 27608, 27609, 27610), # Ukranian forces capture\n",
    "      (26387, 27788), # Will Tesla increase deliveries in Q3 2024\n",
    "      (26821, 26959), # VP debate\n",
    "      (26212, 26213, 26214), # number of dairy cow herds with H5N1\n",
    "      (26639, 26640, 26641) # Presidential debate 0, 1, or 2+\n",
    "  ]\n",
    "\n",
    "  ####### CREATE QUESTION WEIGHTS #########\n",
    "\n",
    "  # Combine both lists of tuples\n",
    "  all_questions = bot_scope_questions + bot_sum_to_1_questions + bot_increasing_questions + bot_similar_questions + bot_conditional_pair\n",
    "\n",
    "  # Create an empty list to store the data\n",
    "  data = []\n",
    "\n",
    "  # Process each tuple\n",
    "  for tuple_questions in all_questions:\n",
    "      # Calculate the weight for each question in the tuple\n",
    "      weight = np.log2(1 + len(tuple_questions))/(1 + len(tuple_questions))\n",
    "\n",
    "      # Add each question and its weight to the data list\n",
    "      for question_id in tuple_questions:\n",
    "          data.append({'bot_question_id': question_id, 'question_weight': weight})\n",
    "\n",
    "  # Process each tuple\n",
    "  for tuple_questions in bot_repeated_questions:\n",
    "      # 1st iteration has weight 1, 2nd has weight 1/2, 3rd weight 1/3....\n",
    "      count = 1\n",
    "\n",
    "      # Add each question and its weight to the data list\n",
    "      for question_id in tuple_questions:\n",
    "          data.append({'bot_question_id': question_id, 'question_weight': 1/count})\n",
    "          count += 1\n",
    "\n",
    "  # Create the DataFrame\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "  # Sort the DataFrame by bot_question_id for better readability\n",
    "  df_bot_question_related_weights = df.sort_values('bot_question_id').reset_index(drop=True)\n",
    "\n",
    "# if df_bot_question_related_weights is defined, replace the question weights in df_pro_bot_resolved_questions\n",
    "if 'df_bot_question_related_weights' in locals():\n",
    "    df_pro_bot_resolved_questions = pd.merge(\n",
    "        df_pro_bot_resolved_questions,\n",
    "        df_bot_question_related_weights,\n",
    "        on='bot_question_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    df_pro_bot_resolved_questions['question_weight'] = df_pro_bot_resolved_questions['question_weight_y'].combine_first(df_pro_bot_resolved_questions['question_weight_x'])\n",
    "    df_pro_bot_resolved_questions.drop(['question_weight_x', 'question_weight_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Are there any non-1 weights (there should be)\n",
    "print(df_pro_bot_resolved_questions[df_pro_bot_resolved_questions['question_weight'] != 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique pro questions, bot questions\n",
    "pro_questions = df_pro_bot_resolved_questions['pro_question_id'].unique()\n",
    "bot_questions = df_pro_bot_resolved_questions['bot_question_id'].unique()\n",
    "print(pro_questions, bot_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pro_bot_resolved_questions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove early closers IF right now is before scheduled close time\n",
    "df_pro_bot_resolved_questions['scheduled_close_time'] = pd.to_datetime(df_pro_bot_resolved_questions['scheduled_close_time']).dt.tz_localize(None)\n",
    "df_pro_bot_resolved_questions['actual_close_time'] = pd.to_datetime(df_pro_bot_resolved_questions['actual_close_time']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_early_closers = False # SET TO FALSE WHEN ALL Q'S ARE RESOLVED\n",
    "if remove_early_closers:\n",
    "  df_pro_bot_resolved_questions = df_pro_bot_resolved_questions[(df_pro_bot_resolved_questions['actual_close_time'] <= df_pro_bot_resolved_questions['scheduled_close_time'])]\n",
    "\n",
    "print('Number of unique questions in df_pro_bot_resolved_questions:', len(df_pro_bot_resolved_questions['bot_question_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read in the scores dataclips from heroku, take last (spot) score for each question_id, forecaster pair; make it into what Tom's code expects\n",
    "\n",
    "## BOTS\n",
    "\n",
    "# BASELINE\n",
    "df_bot_baseline = df_bot_scores[df_bot_scores['score_type'] == 'spot_baseline']\n",
    "\n",
    "# Take the LAST score for each (forecaster, question_id) pair\n",
    "df_bot_baseline = df_bot_baseline.groupby(['question_id', 'forecaster']).last().reset_index()\n",
    "\n",
    "# PEER\n",
    "df_bot_peer = df_bot_scores[df_bot_scores['score_type'] == 'spot_peer']\n",
    "\n",
    "# Take the LAST score for each (forecaster, question_id) pair\n",
    "df_bot_peer = df_bot_peer.groupby(['question_id', 'forecaster']).last().reset_index()\n",
    "\n",
    "## PROS\n",
    "\n",
    "# BASELINE\n",
    "df_pro_baseline = df_pro_scores[df_pro_scores['score_type'] == 'spot_baseline']\n",
    "\n",
    "# Take the LAST score for each (forecaster, question_id) pair\n",
    "df_pro_baseline = df_pro_baseline.groupby(['question_id', 'forecaster']).last().reset_index()\n",
    "df_pro_baseline_long = df_pro_baseline.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ CONVERT BASELINE SCORE TO FORECAST ###################\n",
    "\n",
    "df_bot_forecasts = convert_baseline_to_forecasts(df_bot_baseline)\n",
    "df_bot_forecasts = df_bot_forecasts.drop('resolution', axis=1)\n",
    "\n",
    "df_bot_forecasts.head()\n",
    "\n",
    "df_pro_forecasts = convert_baseline_to_forecasts(df_pro_baseline)\n",
    "df_pro_forecasts = df_pro_forecasts.drop('resolution', axis=1)\n",
    "\n",
    "df_pro_forecasts.head()\n",
    "\n",
    "\n",
    "df_bot_forecasts = add_is_median(df_bot_forecasts)\n",
    "df_pro_forecasts = add_is_median(df_pro_forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_bot_forecasts = add_median_rows(df_bot_forecasts, 'bot')\n",
    "df_pro_forecasts = add_median_rows(df_pro_forecasts, 'pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD THE BOT MEDIAN SPOT SCORES & REMOVE UNNECESSARY COLUMNS\n",
    "df_bot_baseline = df_bot_baseline[['question_id', 'question_title', 'question_weight', 'forecaster', 'score', 'resolution']]\n",
    "\n",
    "# Add bot_median rows\n",
    "df_bot_baseline = df_bot_baseline.merge(df_bot_forecasts[['question_id', 'forecaster', 'is_median']], on=['question_id', 'forecaster'], how='left')\n",
    "df_bot_baseline = add_median_rows(df_bot_baseline, 'bot')\n",
    "\n",
    "df_bot_baseline_long = df_bot_baseline.copy()\n",
    "\n",
    "# DO THE SAME FOR DF_BOT_PEER\n",
    "df_bot_peer = df_bot_peer[['question_id', 'question_title', 'question_weight', 'forecaster', 'score', 'resolution']]\n",
    "\n",
    "# Add bot_median rows\n",
    "df_bot_peer = df_bot_peer.merge(df_bot_forecasts[['question_id', 'forecaster', 'is_median']], on=['question_id', 'forecaster'], how='left')\n",
    "df_bot_peer = add_median_rows(df_bot_peer, 'bot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bots = df_bot_peer['forecaster'].unique()\n",
    "all_bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show results\n",
    "ranked_forecasters = calculate_weighted_stats(df_bot_peer)\n",
    "\n",
    "display_head_and_tail(ranked_forecasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmAFBHIhK77X"
   },
   "outputs": [],
   "source": [
    "# @title Create df_bot_baseline, df_bot_peer, df_bot_forecasts, bots\n",
    "\n",
    "\"\"\"\n",
    "df_bot_baseline: Spot Baseline scores for all bots & bot_median\n",
    "\n",
    "df_bot_peer: Spot Peer scores for all bots & bot_median. Can be used to recreate\n",
    "the tournament leaderboard on the site.\n",
    "\n",
    "df_bot_forecasts: Spot forecasts for all bots & bot_median, ie only counts the\n",
    "final forecast\n",
    "\n",
    "bots: a list of all bots\n",
    "\"\"\"\n",
    "\n",
    "# Pivot df_bot_baseline\n",
    "df_bot_baseline = df_bot_baseline.rename(columns={'question_id': 'bot_question_id'})\n",
    "#df_bot_baseline['score'] = pd.to_numeric(df_bot_baseline['score'], errors='coerce')\n",
    "df_pivoted = df_bot_baseline.pivot(index='bot_question_id', columns='forecaster', values='score')\n",
    "df_pivoted = df_pivoted.reset_index()\n",
    "df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n",
    "\n",
    "# Move 'question_id' to be the first column\n",
    "cols = df_pivoted.columns.tolist()\n",
    "cols = ['bot_question_id'] + [col for col in cols if col != 'bot_question_id']\n",
    "df_pivoted = df_pivoted[cols]\n",
    "\n",
    "all_columns = df_pivoted.columns.tolist()\n",
    "# Remove 'question_id' and 'bot_median' from the list if they exist\n",
    "all_columns = [col for col in all_columns if col not in ['bot_question_id', 'bot_median']]\n",
    "new_column_order = ['bot_question_id', 'bot_median'] + all_columns\n",
    "df_pivoted = df_pivoted[new_column_order]\n",
    "df_bot_baseline_wide = df_pivoted\n",
    "df_bot_baseline_wide['bot_question_id'] = pd.to_numeric(df_bot_baseline_wide['bot_question_id'], errors='coerce')\n",
    "\n",
    "# Create df_bot_peer\n",
    "df_bot_peer = df_bot_peer.rename(columns={'question_id': 'bot_question_id'})\n",
    "df_bot_peer['score'] = pd.to_numeric(df_bot_peer['score'], errors='coerce')\n",
    "\n",
    "df_bot_peer_wide = make_wide(df_bot_peer, df_pro_bot_resolved_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bot_baseline_wide.shape)\n",
    "\n",
    "array_fewer = np.array([28922, 28923, 28924, 28932, 28933, 28934, 28935, 28936, 28937, 28938, 28958, 28959, 28960, 28985, 28986, 28987, 28988, 28989, 28990, 28991, 28992, 28993, 28994, 28995, 29021, 29022, 29023, 29024, 29025, 29026, 29027, 29028, 29029, 29030, 29031, 29032, 29033, 29034, 29035, 29036, 29037, 29066, 29067, 29068, 29069, 29070, 29071, 29072, 29074, 29075, 29077, 29108, 29109, 29110, 29111, 29112, 29113, 29114, 29115, 29116, 29117, 29119, 29162, 29163, 29164, 29165, 29166, 29168, 29169, 29170, 29171, 29172, 29173, 29174, 29175, 29176, 29177, 29200, 29201, 29202, 29203, 29204, 29205, 29206, 29207, 29208, 29209, 29210, 29211, 29248, 29249, 29250, 29251, 29252, 29253, 29254, 29255, 29256, 29257, 29258, 29296, 29297, 29298, 29299, 29300, 29301, 29302, 29303, 29304, 29305, 29306, 29347, 29348, 29349, 29350, 29351, 29353, 29354, 29355, 29356, 29358, 29359, 29360, 29361, 29362, 29414, 29415, 29417, 29418, 29419, 29420, 29421, 29422, 29423, 29460, 29461, 29462, 29463, 29464, 29465, 29466, 29467, 29471, 29472, 29503, 29504, 29505, 29506, 29507, 29508, 29510, 29511, 29512, 29513, 29556, 29557, 29558, 29559, 29560, 29561, 29562, 29563, 29564, 29565, 29566, 29567, 29568, 29569, 29635, 29636, 29637, 29638, 29639, 29640, 29641, 29642, 29643, 29644, 29645, 29646, 29647, 29648, 29649, 29650, 29714, 29715, 29716, 29717, 29718, 29719, 29720, 29721, 29722, 29723, 29724, 29725, 29726, 29727, 29728, 29729, 29771, 29773, 29774, 29775, 29776, 29777, 29778, 29779, 29780, 29781, 29828, 29829, 29830, 29831, 29832, 29833, 29834, 29835, 29836, 29837, 29838, 29839, 29840, 29908, 29909, 29910, 29911, 29912, 29913, 29914, 29915, 29916, 29917, 29940, 29941, 29942, 29943, 29944, 29945, 29946, 29947, 29948, 29949, 29950, 29951, 29952, 29953, 29954, 29985, 29987, 29988, 29989, 29990, 29991, 29992, 29993, 29994, 29995, 29996, 29997, 29998, 30079, 30080, 30081, 30082, 30083, 30084, 30085, 30086, 30087, 30088, 30089, 30090, 30091, 30120, 30121, 30122, 30123, 30124, 30125, 30126, 30127, 30154, 30155, 30156, 30157, 30158, 30159, 30160, 30161, 30162, 30193, 30194, 30196, 30197, 30198, 30199, 30200, 30248, 30250, 30251, 30252, 30253, 30254, 30255, 30256, 30257, 30281, 30282, 30283, 30284, 30285, 30286, 30287, 30288, 30289, 30290, 30317, 30318, 30320, 30321, 30322, 30323, 30324, 30348, 30349, 30350, 30351, 30352, 30353, 30385, 30386, 30387, 30388, 30389, 30392, 30393, 30394, 30395, 30435, 30437, 30438, 30439, 30440, 30441, 30442, 30443, 30444, 30445, 30446, 30447, 30496, 30497, 30498, 30499, 30500, 30501, 30502, 30503, 30504, 30505, 30532, 30533, 30534, 30535, 30536, 30537, 30576, 30577, 30578, 30579, 30580, 30581, 30582, 30583, 30584, 30585, 30586, 30587, 30613, 30614, 30615, 30617, 30637, 30638, 30639, 30640, 30641, 30723, 30724, 30725, 30726, 30740, 30741, 30787, 30791, 30792, 30793, 30794, 30795, 30796, 30797])\n",
    "\n",
    "# List all questions in df_bot_baseline_wide\n",
    "array_new = df_bot_baseline_wide['bot_question_id'].unique()\n",
    "\n",
    "# What's the difference? between questions pre-median fix and questions now (more now):\n",
    "diff = set(array_new) - set(array_fewer)\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XceLWcgCPNw-"
   },
   "outputs": [],
   "source": [
    "# @title Bot Baseline Leaderboard\n",
    "\n",
    "# Calculate the total score for each bot\n",
    "total_scores = df_bot_baseline_wide.iloc[:, 1:].fillna(0).sum()\n",
    "\n",
    "# Create a new dataframe with the total scores\n",
    "df_total_scores = pd.DataFrame({'Bot': total_scores.index, 'Baseline_Score': total_scores.values})\n",
    "\n",
    "# Sort the dataframe by Total_Score in descending order\n",
    "df_total_scores_sorted = df_total_scores.sort_values('Baseline_Score', ascending=False)\n",
    "\n",
    "# Add a Rank column\n",
    "df_total_scores_sorted['Rank'] = range(1, len(df_total_scores_sorted) + 1)\n",
    "\n",
    "# Set Rank as the index\n",
    "df_total_scores_ranked = df_total_scores_sorted.set_index('Rank')\n",
    "\n",
    "# Display the result\n",
    "display_head_and_tail(df_total_scores_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure df_bot_baseline_wide has ONE forecast per (forecaster, question_id) pair\n",
    "# Check for duplicates\n",
    "print(df_bot_baseline_wide.duplicated(subset=['bot_question_id', 'bot_median']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iRDMoH7hTBEq"
   },
   "outputs": [],
   "source": [
    "# @title Bot Peer Leaderboard\n",
    "\n",
    "\"\"\"\n",
    "NOTE: This can be different from the leaderboad on the site IF early closers\n",
    "are excluded (check remove_early_closers bool).\n",
    "\"\"\"\n",
    "\n",
    "df_filled = df_bot_peer_wide.fillna(0)\n",
    "#df_filled = df_filled.drop(['bot_question_id', 'question_weight'], axis=1)\n",
    "\n",
    "# Calculate the total score for each player\n",
    "total_scores = df_filled.sum()\n",
    "\n",
    "# Create a new DataFrame for the leaderboard\n",
    "leaderboard = pd.DataFrame({\n",
    "    'bot': total_scores.index,\n",
    "    'Peer Score': total_scores.values\n",
    "})\n",
    "\n",
    "# Remove bot_question_id from the leaderboard\n",
    "leaderboard = leaderboard[leaderboard['bot'] != 'bot_question_id']\n",
    "\n",
    "# Sort the leaderboard by Total Score in descending order\n",
    "leaderboard = leaderboard.sort_values('Peer Score', ascending=False)\n",
    "\n",
    "# Reset the index and add a 'Rank' column\n",
    "leaderboard = leaderboard.reset_index(drop=True)\n",
    "leaderboard.index += 1\n",
    "leaderboard.index.name = 'Rank'\n",
    "\n",
    "# Display the leaderboard\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD PRO_MEDIAN ROWS\n",
    "\n",
    "# ADD THE PRO MEDIAN SPOT SCORES & REMOVE UNNECESSARY COLUMNS\n",
    "df_pro_scores = df_pro_baseline\n",
    "df_pro_baseline = df_pro_baseline[['question_id', 'question_title', 'question_weight', 'forecaster', 'score', 'resolution']]\n",
    "\n",
    "# Add pro_median rows\n",
    "df_pro_baseline = df_pro_baseline.merge(df_pro_forecasts[['question_id', 'forecaster', 'is_median']], on=['question_id', 'forecaster'], how='left')\n",
    "df_pro_baseline = add_median_rows(df_pro_baseline, 'pro')\n",
    "\n",
    "df_pro_baseline_long = df_pro_baseline.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bot_scores = df_bot_scores[df_bot_scores['score_type'] == 'spot_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print WEIGHTED average for pro_median\n",
    "print(\"PRO MEDIAN\")\n",
    "pro_median_baseline = df_pro_baseline_long[df_pro_baseline_long['forecaster'] == 'pro_median']\n",
    "print(f'Average baseline: {(pro_median_baseline['score'] * pro_median_baseline['question_weight']).sum() / pro_median_baseline['question_weight'].sum()}')\n",
    "\n",
    "# Same for pgodzinai in df_bot_scores (this differs from the bot team results later on because it's on ALL his questions)\n",
    "print(\"pgodzinai MEDIAN\")\n",
    "pgodzinai_baseline = df_bot_scores[df_bot_scores['forecaster'] == 'pgodzinai']\n",
    "print(f'Average baseline: {(pgodzinai_baseline['score'] * pgodzinai_baseline['question_weight']).sum() / pgodzinai_baseline['question_weight'].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgodzinai_baseline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pgodzinai_baseline to only those questions that are also pro questions\n",
    "pgodzinai_baseline = pd.merge(pgodzinai_baseline, df_pro_bot_resolved_questions[['pro_question_id', 'bot_question_id']], left_on='question_id', right_on='bot_question_id', how='left')\n",
    "pgodzinai_baseline = pgodzinai_baseline[~pgodzinai_baseline['pro_question_id'].isna()]\n",
    "print(f'pgodzinai average baseline on pro questions: {(pgodzinai_baseline['score'] * pgodzinai_baseline['question_weight']).sum() / pgodzinai_baseline['question_weight'].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Yfq0_lDKAMl7"
   },
   "outputs": [],
   "source": [
    "# @title Create df_pro_bot_forecasts, df_bot_vs_pro_peer, df_bot_vs_pro_leaderboard, df_bot_vs_pro_weighted_leaderboard\n",
    "\n",
    "\"\"\"\n",
    "df_pro_bot_forecasts: Spot forecasts for all bots & pro_median, question resolutions, and question weights\n",
    "\n",
    "df_bot_vs_pro_peer: Calculates Peer scores as if there is a tournament with only\n",
    "a single bot and the pro_median. This is the main comparison metric for\n",
    "assessing how a bot compares to the human aggregate. Positive scores mean that\n",
    "the bot did better than the pro_median. Negative scores mean that the bot did\n",
    "worse than the pro_median.\n",
    "\n",
    "df_bot_vs_pro_leaderboard: A leaderboard based on df_bot_vs_pro_peer.\n",
    "\n",
    "df_bot_vs_pro_weighted_leaderboard: A leaderboard based on df_bot_vs_pro_peer\n",
    "with question weighting.\n",
    "\"\"\"\n",
    "\n",
    "# Now pivot df_pro_forecasts; forecaster = columns; forecast = values; index = pro_question_id\n",
    "df_pro_forecasts = df_pro_forecasts.rename(columns={'question_id': 'pro_question_id'})\n",
    "df_pro_forecasts = df_pro_forecasts.pivot(index='pro_question_id', columns='forecaster', values='forecast')\n",
    "# Make the index a column and make it numeric\n",
    "df_pro_forecasts = df_pro_forecasts.reset_index()\n",
    "\n",
    "# Now pivot df_bot_forecasts; forecaster = columns; forecast = values; index = pro_question_id\n",
    "df_bot_forecasts = df_bot_forecasts.rename(columns={'question_id': 'bot_question_id'})\n",
    "df_bot_forecasts = df_bot_forecasts.pivot(index='bot_question_id', columns='forecaster', values='forecast')\n",
    "# Make the index a column and make it numeric\n",
    "df_bot_forecasts = df_bot_forecasts.reset_index()\n",
    "\n",
    "# One row per question, with pro_question_id and bot_question_id and resolution\n",
    "df_pro_bot_resolved_questions_first = df_pro_bot_resolved_questions.groupby(['pro_question_id', 'bot_question_id']).first().reset_index()[['pro_question_id', 'bot_question_id', 'resolution', 'question_weight']]\n",
    "\n",
    "df2 = pd.merge(\n",
    "    df_pro_bot_resolved_questions_first,\n",
    "    df_pro_forecasts[['pro_question_id', 'pro_median']],\n",
    "    on='pro_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_forecasts = pd.merge(\n",
    "    df2,\n",
    "    df_bot_forecasts,\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "display_head_and_tail(df_pro_bot_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store peer scores\n",
    "df_bot_vs_pro_peer = df_pro_bot_forecasts.copy()\n",
    "df_bot_vs_pro_peer = df_bot_vs_pro_peer[['pro_median', 'resolution', 'question_weight', 'bot_question_id']]\n",
    "\n",
    "# Calculate peer score for each bot\n",
    "for bot in all_bots:\n",
    "    # Calculate Head-to-head score based on the condition\n",
    "    peer_score = np.where(\n",
    "        df_pro_bot_forecasts['resolution'] == 'yes',\n",
    "        np.log(df_pro_bot_forecasts[bot] / df_pro_bot_forecasts['pro_median']),\n",
    "        np.log((1 - df_pro_bot_forecasts[bot]) / (1 - df_pro_bot_forecasts['pro_median']))\n",
    "    )\n",
    "\n",
    "    # Add the calculated peer score to the new DataFrame\n",
    "    df_bot_vs_pro_peer[bot] = 100 * peer_score\n",
    "\n",
    "# Calculate Head-to-head score for bot_team\n",
    "peer_score = np.where(\n",
    "    df_pro_bot_forecasts['resolution'] == 'yes',\n",
    "    np.log(df_pro_bot_forecasts['bot_median'] / df_pro_bot_forecasts['pro_median']),\n",
    "    np.log((1 - df_pro_bot_forecasts['bot_median']) / (1 - df_pro_bot_forecasts['pro_median']))\n",
    ")\n",
    "\n",
    "# Add the calculated peer score to the new DataFrame\n",
    "df_bot_vs_pro_peer[\"bot_team_median\"] = 100 * peer_score\n",
    "\n",
    "display_head_and_tail(df_bot_vs_pro_peer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average pro median forecast on questions that resolved yes/no vs top bot\n",
    "\n",
    "top_bot = leaderboard['bot'][1]\n",
    "\n",
    "resolved_yes = df_pro_bot_forecasts[df_pro_bot_forecasts['resolution'] == 'yes']\n",
    "resolved_no = df_pro_bot_forecasts[df_pro_bot_forecasts['resolution'] == 'no']\n",
    "\n",
    "# Calculate the average pro median forecast for questions that resolved yes\n",
    "mean_pro_median_yes = resolved_yes['pro_median'].mean().round(2) * 100\n",
    "mean_pro_median_no = resolved_no['pro_median'].mean().round(2) * 100\n",
    "\n",
    "mean_bot_yes = resolved_yes[top_bot].mean().round(2) * 100\n",
    "mean_bot_no = resolved_no[top_bot].mean().round(2) * 100\n",
    "\n",
    "print(f'mean pro median forecast on questions that resolved yes: {mean_pro_median_yes}%')\n",
    "print(f'mean pro median forecast on questions that resolved no: {mean_pro_median_no}%')\n",
    "print(f'mean {top_bot} forecast on questions that resolved yes: {mean_bot_yes}%')\n",
    "print(f'mean {top_bot} forecast on questions that resolved no: {mean_bot_no}%')\n",
    "\n",
    "# Plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create x-coordinates with jitter for each group separately\n",
    "x_bot_yes = np.random.normal(0, 0.04, len(resolved_yes))\n",
    "x_pro_yes = np.random.normal(1, 0.04, len(resolved_yes))\n",
    "x_bot_no = np.random.normal(0, 0.04, len(resolved_no))\n",
    "x_pro_no = np.random.normal(1, 0.04, len(resolved_no))\n",
    "\n",
    "# Plot points for \"yes\" resolution\n",
    "plt.scatter(x_bot_yes, resolved_yes['pro_median'] * 100, \n",
    "           color='blue', alpha=0.6, label='Resolved Yes')\n",
    "plt.scatter(x_pro_yes, resolved_yes[top_bot] * 100, \n",
    "           color='blue', alpha=0.6)\n",
    "\n",
    "# Plot points for \"no\" resolution\n",
    "plt.scatter(x_bot_no, resolved_no['pro_median'] * 100, \n",
    "           color='red', alpha=0.6, label='Resolved No')\n",
    "plt.scatter(x_pro_no, resolved_no[top_bot] * 100, \n",
    "           color='red', alpha=0.6)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xticks([0, 1], ['pro_median', top_bot])\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.title('Pro Median vs Top Bot Forecasts')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Set y-axis limits from 0 to 100\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = df_bot_vs_pro_peer.sum(axis=0)\n",
    "# remove resolution, question_weight, bot_question_id from total scores\n",
    "total_scores = total_scores.drop(['resolution', 'question_weight', 'bot_question_id'])\n",
    "\n",
    "# First pivot to long format - each row will be a question-forecaster pair\n",
    "df_long = df_bot_vs_pro_peer.melt(\n",
    "    id_vars=['bot_question_id', 'question_weight', 'resolution'],\n",
    "    var_name='forecaster',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Drop any rows where score is NaN\n",
    "df_long = df_long.dropna(subset=['score'])\n",
    "\n",
    "# Cast question_weight as numeric\n",
    "df_long['question_weight'] = pd.to_numeric(df_long['question_weight'], errors='coerce')\n",
    "\n",
    "# Group first, then do the multiplication and sum\n",
    "weighted_scores = df_long.groupby('forecaster').apply(lambda x: (x['score'] * x['question_weight']).sum(axis=0))\n",
    "\n",
    "# Calculate number of questions answered by each bot\n",
    "num_questions = df_long.groupby('forecaster')['bot_question_id'].nunique()\n",
    "#num_weighted_questions = df_bot_vs_pro_peer.mul(df_pro_bot_forecasts['question_weight'], axis=0).apply(lambda col: col[col.notna() & col.apply(np.isreal)].count())\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "results = pd.DataFrame({\n",
    "    'Peer_vs_Pro': total_scores,\n",
    "    'Count': num_questions\n",
    "})\n",
    "\n",
    "weighted_results = pd.DataFrame({\n",
    "    'W_Peer_vs_Pro': weighted_scores,\n",
    "    'Count': num_questions\n",
    "})\n",
    "\n",
    "df_bot_vs_pro_leaderboard = results.sort_values(by='Peer_vs_Pro', ascending=False)\n",
    "df_bot_vs_pro_weighted_leaderboard = weighted_results.sort_values(by='W_Peer_vs_Pro', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pro_baseline = df_pro_baseline.rename(columns={'question_id': 'pro_question_id'})\n",
    "df_pro_baseline = df_pro_baseline[['pro_question_id', 'forecaster', 'score']]\n",
    "\n",
    "# Now make it wide! forecaster = columns; score = values; index = pro_question_id\n",
    "df_pro_baseline_wide = df_pro_baseline.pivot(index='pro_question_id', columns='forecaster', values='score').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tXKRpXAVHMRt"
   },
   "outputs": [],
   "source": [
    "# @title Create df_pro_bot_baseline_leaderboard, df_pro_bot_baseline_weighted_leaderboard\n",
    "\n",
    "df_pro_bot_baseline_weights = pd.merge(\n",
    "    df_pro_bot_resolved_questions,\n",
    "    df_bot_baseline_wide,\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_pro_bot_baseline_weights = pd.merge(\n",
    "    df_pro_bot_baseline_weights,\n",
    "    df_pro_baseline_wide[['pro_question_id', 'pro_median']],\n",
    "    on='pro_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove rows where pro_question_id is NaN (only want overlapping questions here)\n",
    "df_pro_bot_baseline_weights = df_pro_bot_baseline_weights.dropna(subset=['pro_question_id'])\n",
    "\n",
    "# Create a list of columns to keep\n",
    "forecaster_cols = ['pro_median'] + [col for col in df_pro_bot_baseline_weights.columns if col in all_bots]\n",
    "df_filtered = df_pro_bot_baseline_weights[forecaster_cols]\n",
    "\n",
    "# Calculate the sum for each forecaster\n",
    "forecaster_scores = df_filtered.sum()\n",
    "forecaster_weighted_scores = df_filtered.mul(df_pro_bot_baseline_weights['question_weight'], axis=0).sum()\n",
    "\n",
    "question_counts = df_filtered.notna().sum()\n",
    "question_weighted_counts = df_filtered.notna().mul(df_pro_bot_baseline_weights['question_weight'], axis=0).sum()\n",
    "\n",
    "# Create a DataFrame for the leaderboard\n",
    "leaderboard = pd.DataFrame({\n",
    "    'Forecaster': forecaster_scores.index,\n",
    "    'Baseline': forecaster_scores.values,\n",
    "    'Count': question_counts.values\n",
    "})\n",
    "\n",
    "# Create a DataFrame for the leaderboard\n",
    "weighted_leaderboard = pd.DataFrame({\n",
    "    'Forecaster': forecaster_weighted_scores.index,\n",
    "    'Weighted_Baseline': forecaster_weighted_scores.values,\n",
    "    'Count': question_counts.values,\n",
    "    'Weighted Count': question_weighted_counts.values\n",
    "})\n",
    "\n",
    "# Sort the leaderboard by score in descending order\n",
    "leaderboard = leaderboard.sort_values('Baseline', ascending=False).reset_index(drop=True)\n",
    "weighted_leaderboard = weighted_leaderboard.sort_values('Weighted_Baseline', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add a 'Rank' column\n",
    "leaderboard['Rank'] = leaderboard.index + 1\n",
    "weighted_leaderboard['Rank'] = weighted_leaderboard.index + 1\n",
    "\n",
    "# Reorder columns to have Rank first\n",
    "leaderboard = leaderboard[['Rank', 'Forecaster', 'Baseline', 'Count']]\n",
    "weighted_leaderboard = weighted_leaderboard[['Rank', 'Forecaster', 'Weighted_Baseline', 'Count', 'Weighted Count']]\n",
    "\n",
    "#leaderboard\n",
    "weighted_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGNedTHmU-Bm",
    "outputId": "a7935679-8993-4329-d05d-fd701c4b77a8"
   },
   "outputs": [],
   "source": [
    "# @title Weighted head-to-head, T test\n",
    "\n",
    "\"\"\"\n",
    "df_W_leaderboard: A leaderboard based on df_bot_vs_pro_peer with question\n",
    "weighting and the calculations for doing a weighted T test\n",
    "\"\"\"\n",
    "\n",
    "forecaster_weighted_scores = forecaster_weighted_scores.fillna(0)\n",
    "\n",
    "# Cast weights as numeric\n",
    "df_bot_vs_pro_peer['question_weight'] = pd.to_numeric(df_bot_vs_pro_peer['question_weight'], errors='coerce')\n",
    "\n",
    "# Calculate weighted statistics for each bot\n",
    "df_W_leaderboard = calculate_t_test(df_bot_vs_pro_peer, all_bots)\n",
    "\n",
    "df_W_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "df_W_leaderboard.to_csv('weighted_t_test_h2h_bot_vs_pros.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d_ZdL0A0qTz",
    "outputId": "e30ee8fb-0faf-45ae-974e-d4af282e0252"
   },
   "outputs": [],
   "source": [
    "# @title Weighted Bot Peer, T test (to compare bots against each other, use ALL QUESTIONS)\n",
    "\n",
    "df_W_bot_peer_leaderboard = pd.DataFrame()\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "forecaster_weighted_scores = forecaster_weighted_scores.fillna(0)\n",
    "\n",
    "# OMIT bot_median column for this bit\n",
    "df_bot_peer_wide_b = df_bot_peer_wide.drop('bot_median', axis=1)\n",
    "df_bot_peer = df_bot_peer[df_bot_peer['forecaster'] != 'bot_median']\n",
    "\n",
    "bots_for_peer = np.array(list(set(df_bot_peer['forecaster'])))\n",
    "\n",
    "df_W_leaderboard = calculate_t_test(df_bot_peer_wide_b, bots_for_peer)\n",
    "\n",
    "df_W_leaderboard_print = df_W_leaderboard.sort_values(by='lower_bound', ascending=False)\n",
    "df_W_leaderboard_print['Rank'] = range(1, len(df_W_leaderboard_print) + 1)\n",
    "\n",
    "# Make index into a column - Bot\n",
    "df_W_leaderboard_print = df_W_leaderboard_print.reset_index()\n",
    "df_W_leaderboard_print = df_W_leaderboard_print.rename(columns={'index': 'Bot'})\n",
    "#df_W_leaderboard_print = df_W_leaderboard_print[['Rank', 'Bot', 'W_ave', 'W_count', 'lower_bound', 'upper_bound']]\n",
    "# Make rank the first column; leave rest the same\n",
    "cols = df_W_leaderboard_print.columns.tolist()\n",
    "cols = ['Rank'] + cols[:-1]\n",
    "df_W_leaderboard_print = df_W_leaderboard_print[cols]\n",
    "\n",
    "df_W_leaderboard_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "df_W_leaderboard_print.to_csv('weighted_bot_peer_leaderboard_t_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "88QO8eyW6T_T",
    "outputId": "e83d6794-13a2-454d-cb70-0a38b065d9e7"
   },
   "outputs": [],
   "source": [
    "# @title Histogram of bot\n",
    "\n",
    "# Extract the 'mf-bot-1' column and remove NaN values\n",
    "\n",
    "name = 'mf-bot-1'\n",
    "\n",
    "scores = df_bot_peer_wide[name].dropna()\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(scores, bins=30, density=True, alpha=0.7, color='skyblue')\n",
    "\n",
    "# Fit a normal distribution to the data\n",
    "mu, std = norm.fit(scores)\n",
    "\n",
    "# Plot the PDF of the fitted normal distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f\"Histogram of {name} Scores with Fitted Gaussian\", fontsize=16)\n",
    "plt.xlabel(\"Score\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "\n",
    "# Add text box with distribution parameters\n",
    "textstr = f'$\\mu={mu:.2f}$\\n$\\sigma={std:.2f}$'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bot_peer_wide.shape\n",
    "\n",
    "display_head_and_tail(df_bot_peer_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxVJxrCpuXV_",
    "outputId": "3df39cbc-b594-40e1-d08f-1b0e9736d6ec"
   },
   "outputs": [],
   "source": [
    "# Drop 'bot_median' from all_bots list\n",
    "all_bots_wo_median = np.delete(all_bots, np.where(all_bots == 'bot_median')[0][0])\n",
    "df_bot_peer_wide_wo_median = df_bot_peer_wide.drop('bot_median', axis=1)\n",
    "\n",
    "\n",
    "NUM = round(df_bot_peer_wide['question_weight'].sum())\n",
    "ITER = 1000\n",
    "\n",
    "result_df = weighted_bootstrap_analysis(df_bot_peer_wide_wo_median, all_bots_wo_median, NUM, ITER)\n",
    "average_df = result_df / NUM\n",
    "\n",
    "print(f'BOT LEADERBOARD\\n\\n')\n",
    "df_rounded = average_df.round(1)\n",
    "df_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "MXAev2sNXdbZ",
    "outputId": "eebb723f-5494-4b89-cf0d-efa5b1626cb7"
   },
   "outputs": [],
   "source": [
    "NUM = round(df_bot_vs_pro_peer['question_weight'].sum())\n",
    "ITER = 1000\n",
    "\n",
    "result_df = weighted_bootstrap_analysis(df_bot_vs_pro_peer, all_bots, NUM, ITER)\n",
    "average_df = result_df / NUM\n",
    "\n",
    "print(f'\\n\\n\\nHEAD-TO-HEAD LEADERBOARD\\n\\n')\n",
    "#df_rounded = result_df.round(0).astype(int)\n",
    "df_rounded = average_df.round(1)\n",
    "\n",
    "df_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df_rounded (bootstrapping h2h) to csv\n",
    "df_rounded.to_csv('bootstrapped_h2h_bot_vs_pros.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Check specific bot records\n",
    "\n",
    "bot_name = 'annabot'\n",
    "\n",
    "df_bot = df_bot_peer_wide[['bot_question_id', 'question_weight', bot_name]]\n",
    "df_bot = df_bot.dropna()\n",
    "df_bot = df_bot.reset_index(drop=True)\n",
    "\n",
    "df_bot['weighted_score'] = df_bot[bot_name] * df_bot['question_weight']\n",
    "\n",
    "weighted_score = df_bot['weighted_score'].sum()\n",
    "\n",
    "print(f\"Weighted score for {bot_name}: {weighted_score}\")\n",
    "\n",
    "total_score = df_bot[bot_name].sum()\n",
    "\n",
    "print(f\"Total score for {bot_name}: {total_score}\\n\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size (optional)\n",
    "plt.hist(df_bot[bot_name], bins=10, edgecolor='black')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f'Histogram of Scores for {bot_name}')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add grid lines (optional)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7W8JXutv2ks",
    "outputId": "5e7053d3-2124-42b7-bd53-48a40a53caf2"
   },
   "outputs": [],
   "source": [
    "# @title Weighted Bot Only Peer, T test\n",
    "\n",
    "# To choose our top bot team, we only use the questions for which there is no Pro benchmark. (in Q4, there were some bots who ONLY forecasted on questions with Pro benchmark)\n",
    "no_pro_benchmark = df_pro_bot_resolved_questions[df_pro_bot_resolved_questions['pro_question_id'].isna()]['bot_question_id']\n",
    "\n",
    "df_bot_only_peer = df_bot_peer[df_bot_peer['bot_question_id'].isin(no_pro_benchmark)]\n",
    "df_bot_only_peer_wide = make_wide(df_bot_only_peer, df_pro_bot_resolved_questions)\n",
    "\n",
    "df_W_bot_only_peer_leaderboard = calculate_t_test(df_bot_only_peer_wide, df_bot_only_peer['forecaster'].unique())\n",
    "\n",
    "df_W_bot_only_peer_leaderboard[['W_ave', 'W_count', 'lower_bound', 'upper_bound', 'p_value']].sort_values(by='lower_bound', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_bot_only_peer_leaderboard.to_csv('weighted_bot_ONLY_peer_leaderboard_t_test.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the lower_bound column in descending order\n",
    "sorted_df = df_W_bot_only_peer_leaderboard.sort_values(by='lower_bound', ascending=False)\n",
    "\n",
    "# exclude bot median for purposes of bot teaming\n",
    "sorted_df = sorted_df.drop('bot_median', errors='ignore')\n",
    "\n",
    "# Get the top 10 bot names\n",
    "top_10_bots = sorted_df.index[:10].tolist()\n",
    "\n",
    "# Print the list of top 10 bots\n",
    "print(\"Top 10 bots:\")\n",
    "for i, bot in enumerate(top_10_bots, 1):\n",
    "    print(f\"{i}. {bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x6e1kZl12qFZ"
   },
   "outputs": [],
   "source": [
    "# @title Calculate df_bot_team_forecasts\n",
    "\n",
    "df_bot_team_forecasts = pd.merge(\n",
    "    df_bot_forecasts,\n",
    "    df_pro_bot_resolved_questions[['bot_question_id', 'pro_question_id', 'question_weight', 'resolution']],\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# KEEP ONLY ROWS WHERE PRO_QUESTION_ID IS NA\n",
    "df_bot_team_forecasts = df_bot_team_forecasts[~df_bot_team_forecasts['pro_question_id'].isna()]\n",
    "\n",
    "columns_to_keep = ['bot_question_id', 'question_weight', 'resolution'] + top_10_bots\n",
    "\n",
    "# Filter the DataFrame to keep only the specified columns\n",
    "df_bot_team_forecasts = df_bot_team_forecasts[columns_to_keep]\n",
    "\n",
    "\n",
    "# Calculate and add median forecasts for 2 to 10 bots\n",
    "for i in range(1, 11):\n",
    "    bots_subset = top_10_bots[:i]\n",
    "    column_name = f'median_forecast_{i}_bots'\n",
    "    df_bot_team_forecasts[column_name] = calculate_median_forecast(df_bot_team_forecasts, bots_subset)\n",
    "\n",
    "display_head_and_tail(df_bot_team_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "a = df_bot_team_forecasts['question_weight'].sum()\n",
    "b = df_bot_team_forecasts.shape[0] # number of rows in df_bot_team_forecasts\n",
    "print(f'Sum of weights: {a}, Number of questions: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-FedHpWV_1v",
    "outputId": "7327c204-c501-4dfb-bdfb-176606c96dc4"
   },
   "outputs": [],
   "source": [
    "# @title Calculate the baseline scores for each team size\n",
    "\n",
    "teams = ['median_forecast_1_bots',\n",
    "         'median_forecast_2_bots',\n",
    "         'median_forecast_3_bots',\n",
    "         'median_forecast_4_bots',\n",
    "         'median_forecast_5_bots',\n",
    "         'median_forecast_6_bots',\n",
    "         'median_forecast_7_bots',\n",
    "         'median_forecast_8_bots',\n",
    "         'median_forecast_9_bots',\n",
    "         'median_forecast_10_bots']\n",
    "\n",
    "weighted_scores = calculate_weighted_scores(df_bot_team_forecasts, teams)\n",
    "\n",
    "# Print nicely - round to 2 decimal places and first column should be just an integer (bot team size)\n",
    "weighted_scores_print = pd.DataFrame(weighted_scores).reset_index()\n",
    "weighted_scores_print.columns = ['Bot_Team_Size', 'Weighted_Baseline_Score_for_Bot_Team_Median']\n",
    "weighted_scores_print['Weighted_Baseline_Score_for_Bot_Team_Median'] = weighted_scores_print['Weighted_Baseline_Score_for_Bot_Team_Median'].round(2)\n",
    "weighted_scores_print['Bot_Team_Size'] = weighted_scores_print['Bot_Team_Size'].apply(lambda x: int(x.split('_')[2].split('_')[0]))\n",
    "weighted_scores_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of top bot team from weighted_scores_print?\n",
    "winning_bot_team_size = weighted_scores_print.sort_values(by='Weighted_Baseline_Score_for_Bot_Team_Median', ascending=False).head(1)['Bot_Team_Size'].values[0]\n",
    "top_bot_team = top_10_bots[:winning_bot_team_size]\n",
    "top_bot_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3TTBVWoZVzU",
    "outputId": "0eb32f2c-09c6-4a15-e81a-bee353b1bccf"
   },
   "outputs": [],
   "source": [
    "# @title Weighted team-vs-pro\n",
    "\n",
    "# We have our top bot team members.\n",
    "# Calculate their median forecast on the pro_bot questions.\n",
    "# Create df with bot_question_id, forecasts, resolution, weights\n",
    "# Calculate the head-to-head score\n",
    "\n",
    "df_top_bot_forecasts = df_bot_forecasts[['bot_question_id'] + top_bot_team]\n",
    "df_top_bot_forecasts['bot_team_median'] = df_top_bot_forecasts[top_bot_team].median(axis=1)\n",
    "\n",
    "df_pro_median = df_pro_forecasts[['pro_question_id', 'pro_median']]\n",
    "\n",
    "df_top_bot_pro_forecasts = pd.merge(\n",
    "    df_pro_bot_resolved_questions,\n",
    "    df_top_bot_forecasts[['bot_question_id', 'bot_team_median']],\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_top_bot_pro_forecasts = pd.merge(\n",
    "    df_top_bot_pro_forecasts,\n",
    "    df_pro_median,\n",
    "    on='pro_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Copy with union (not just overlapping questions)\n",
    "df_top_bot_pro_forecasts_all = df_top_bot_pro_forecasts.copy()\n",
    "\n",
    "# Filter to only those rows where pro_median is not NA\n",
    "df_top_bot_pro_forecasts = df_top_bot_pro_forecasts.dropna(subset=['pro_median'])\n",
    "\n",
    "\n",
    "# Add the head_to_head column\n",
    "df_top_bot_pro_forecasts['head_to_head'] = df_top_bot_pro_forecasts.apply(calculate_head_to_head, args=('bot_team_median', 'pro_median'), axis=1)\n",
    "\n",
    "df_top_bot_pro_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_total_score = get_weighted_score(df_top_bot_pro_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "JlU9zyqn26Rl",
    "outputId": "ac54d636-670b-4a8f-aea9-402679efacf9"
   },
   "outputs": [],
   "source": [
    "plot_head_to_head_distribution(df_top_bot_pro_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1qC4m2VefLe",
    "outputId": "2f110b55-caf6-4ea8-9dfe-b746c3e4d892"
   },
   "outputs": [],
   "source": [
    "df_bot_team_h2h = calculate_t_test(df_top_bot_pro_forecasts, ['head_to_head'])\n",
    "\n",
    "df_bot_team_h2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0I0myCHpl7FT",
    "outputId": "bcc45b9a-f328-4f0c-ef98-a7620af7e358"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "df_sorted = df_top_bot_pro_forecasts.sort_values(by='head_to_head')\n",
    "# Round to four decimal places and format as percent\n",
    "df_sorted['bot_team_median'] = df_sorted['bot_team_median'].map(\"{:.1%}\".format)\n",
    "df_sorted['pro_median'] = df_sorted['pro_median'].map(\"{:.1%}\".format)\n",
    "df_sorted['head_to_head'] = df_sorted['head_to_head'].round(1)\n",
    "#df_sorted['resolution'] = df_sorted['resolution'].map({1: 'yes', 0: 'no'})\n",
    "\n",
    "df_top5 = df_sorted.head(5)\n",
    "df_bottom5 = df_sorted.tail(5)\n",
    "\n",
    "print(\"Top 5:\")\n",
    "\n",
    "df_top5[['title', 'bot_team_median', 'pro_median', 'resolution', 'head_to_head']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBottom 5:\")\n",
    "\n",
    "df_bottom5[['title', 'bot_team_median', 'pro_median', 'resolution', 'head_to_head']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast df_top_bot_pro_forecasts['resolution'] as string - idk why this is necessary but it is\n",
    "df_top_bot_pro_forecasts['resolution'] = df_top_bot_pro_forecasts['resolution'].astype(pd.StringDtype())\n",
    "df_top_bot_pro_forecasts['resolution'] = df_top_bot_pro_forecasts['resolution'].map({'yes': 1, 'no': 0})\n",
    "df_top_bot_pro_forecasts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "BjNQ4IND6Ct7",
    "outputId": "c0ec1316-ef4e-4bd1-875d-148b65ba0114"
   },
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "\n",
    "# Plot calibration curves for bot_team_median and pro_median\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts, 'bot_team_median', 'Bot Team Median', 'blue')\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts, 'pro_median', 'Pro Median', 'red')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Assigned Probability', fontsize=12)\n",
    "plt.ylabel('Fraction that Resolved \\'Yes\\'', fontsize=12)\n",
    "plt.title(f'Calibration Curve: Bot Team Median vs Pro Median\\n(only overlap: {len(df_top_bot_pro_forecasts)} questions)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Number of pro forecasts: {len(df_top_bot_pro_forecasts)}\")\n",
    "print(f\"Number of bot forecasts: {len(df_bot_forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map resolution to 0 and 1\n",
    "df_top_bot_pro_forecasts_all['resolution'] = df_top_bot_pro_forecasts_all['resolution'].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "\n",
    "# Plot calibration curves for bot_team_median and pro_median\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts_all, 'bot_team_median', 'Bot Team Median', 'blue')\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts, 'pro_median', 'Pro Median', 'red')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Assigned Probability', fontsize=12)\n",
    "plt.ylabel('Fraction that Resolved \\'Yes\\'', fontsize=12)\n",
    "plt.title(f'Calibration Curve: Bot Team Median vs Pro Median\\n(all questions)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Number of pro forecasts: {len(df_top_bot_pro_forecasts)}\")\n",
    "print(f\"Number of bot forecasts: {len(df_bot_forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPPgorXB7omi",
    "outputId": "24571b16-50b7-4e51-cd3d-420c15c7fe42"
   },
   "outputs": [],
   "source": [
    "# Calculate confidence scores for bot_team_median and pro_median\n",
    "bot_confidence = calculate_confidence(df_top_bot_pro_forecasts['bot_team_median'], df_top_bot_pro_forecasts['resolution'])\n",
    "pro_confidence = calculate_confidence(df_top_bot_pro_forecasts['pro_median'], df_top_bot_pro_forecasts['resolution'])\n",
    "\n",
    "print(f\"Bot team confidence score: {bot_confidence:.4f}\")\n",
    "print(f\"Pro team confidence score: {pro_confidence:.4f}\")\n",
    "\n",
    "print(f\"Bot team is {interpret_confidence(bot_confidence)}\")\n",
    "print(f\"Pro team is {interpret_confidence(pro_confidence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N26JZjCV9_jc",
    "outputId": "eacb7626-54d0-47c7-8f21-48e95e709564"
   },
   "outputs": [],
   "source": [
    "# Call the function with your DataFrame and column names\n",
    "create_discrimination_histogram(df_top_bot_pro_forecasts,\n",
    "                                'bot_team_median',\n",
    "                                'pro_median',\n",
    "                                'resolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dkNBotk_4e3",
    "outputId": "d393a72e-997a-4025-ca7b-6f5328436286"
   },
   "outputs": [],
   "source": [
    "# Calculate average forecasts for resolved 1 and 0 for bots\n",
    "bot_avg_1 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 1]['bot_team_median'].mean()\n",
    "bot_avg_0 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 0]['bot_team_median'].mean()\n",
    "\n",
    "# Calculate average forecasts for resolved 1 and 0 for pros\n",
    "pro_avg_1 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 1]['pro_median'].mean()\n",
    "pro_avg_0 = df_top_bot_pro_forecasts[df_top_bot_pro_forecasts['resolution'] == 0]['pro_median'].mean()\n",
    "\n",
    "# Calculate the differences\n",
    "bot_difference = bot_avg_1 - bot_avg_0\n",
    "pro_difference = pro_avg_1 - pro_avg_0\n",
    "\n",
    "print(f\"Bot average forecast difference (1 - 0): {bot_difference:.4f}\")\n",
    "print(f\"Pro average forecast difference (1 - 0): {pro_difference:.4f}\")\n",
    "\n",
    "# Calculate the difference between pro and bot differences\n",
    "pro_bot_difference = pro_difference - bot_difference\n",
    "print(f\"Difference between pro and bot differences: {pro_bot_difference:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGnXswWOx_yw",
    "outputId": "35a0e2a8-5831-43cf-a006-f8e0262666ec"
   },
   "outputs": [],
   "source": [
    "# Calculate weighted number of 1 resolutions\n",
    "weighted_ones = np.sum(\n",
    "    df_top_bot_pro_forecasts['resolution'] *\n",
    "    df_top_bot_pro_forecasts['question_weight']\n",
    ")\n",
    "\n",
    "# Calculate weighted number of 0 resolutions\n",
    "weighted_zeros = np.sum(\n",
    "    (1 - df_top_bot_pro_forecasts['resolution']) *\n",
    "    df_top_bot_pro_forecasts['question_weight']\n",
    ")\n",
    "\n",
    "print(f\"Weighted number of 1 resolutions: {weighted_ones}\")\n",
    "print(f\"Weighted number of 0 resolutions: {weighted_zeros}\")\n",
    "\n",
    "print(f\"Average 1 resolutions: {weighted_ones / (weighted_zeros + weighted_ones)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CP COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CP COMPARISON\n",
    "\n",
    "cp = pd.read_csv('https://data.heroku.com/dataclips/xwbtczmsuszvlbrhdifhsilplfxf.csv')\n",
    "cp.rename(columns={'post_id': 'cp_post_id', 'question_id': 'cp_question_id'}, inplace=True)\n",
    "\n",
    "bot_cp_id = pd.read_csv('bot_to_main_feed_ids.csv')\n",
    "                     \n",
    "# Merge these on cp_question_id\n",
    "df_bot_cp = pd.merge(bot_cp_id, cp, on='cp_post_id', how='right') # ahh?\n",
    "\n",
    "df_bot_cp = df_bot_cp[df_bot_cp['bot_question_id'].notnull()]\n",
    "df_bot_cp['bot_question_id'] = df_bot_cp['bot_question_id'].astype(int)\n",
    "\n",
    "# Evaluate cp_reveal_time, start_time, and end_time as datetime objects\n",
    "df_bot_cp['cp_reveal_time'] = pd.to_datetime(df_bot_cp['cp_reveal_time'])\n",
    "df_bot_cp['start_time'] = pd.to_datetime(df_bot_cp['start_time'])\n",
    "df_bot_cp['end_time'] = pd.to_datetime(df_bot_cp['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each group of (bot_question_id, question_title, cp_reveal_time), take only the row with the start_time closest to (BUT LESS THAN) cp_reveal_time\n",
    "df_bot_cp = df_bot_cp.sort_values(by=['bot_question_id', 'cp_reveal_time', 'start_time'])\n",
    "df_bot_cp = df_bot_cp[df_bot_cp['start_time'] < df_bot_cp['cp_reveal_time']]\n",
    "df_bot_cp = df_bot_cp.drop_duplicates(subset=['bot_question_id', 'cp_reveal_time', 'title'], keep='last')\n",
    "\n",
    "## Convert string representation of lists to actual lists\n",
    "df_bot_cp['forecast_values'] = df_bot_cp['forecast_values'].str.strip('[]').str.split(',').apply(lambda x: [float(i.strip()) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deal with influenza rows by filtering to rows where dates match.\n",
    "\n",
    "# First do the December/November replacements only on the relevant rows\n",
    "mask_29507 = df_bot_cp['cp_post_id'] == 29507\n",
    "df_bot_cp.loc[mask_29507, 'title'] = df_bot_cp.loc[mask_29507, 'title'].str.replace('December', 'Dec', regex=False)\n",
    "df_bot_cp.loc[mask_29507, 'title'] = df_bot_cp.loc[mask_29507, 'title'].str.replace('November', 'Nov', regex=False)\n",
    "\n",
    "# Then filter only those rows by the date matching condition\n",
    "matching_rows = df_bot_cp[mask_29507].apply(lambda row:\n",
    "    row['title'].find(re.search(r'(\\w+ \\d+)', row['question_title']).group(1)) != -1,\n",
    "    axis=1)\n",
    "\n",
    "# Update only the matching rows within the 29507 subset\n",
    "df_bot_cp = pd.concat([df_bot_cp[~mask_29507], df_bot_cp[mask_29507][matching_rows]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down by types - \"group\" (multiple choice; my bad), \"binary\" and \"numeric\"\n",
    "\n",
    "# Group questions are the ones that have NON-EMPTY lists in the options column\n",
    "groups = df_bot_cp[df_bot_cp['type'] == 'multiple_choice']\n",
    "groups['options'] = groups['options'].str.strip('[]').str.split(',').apply(lambda x: [i.strip().strip(\"'\") for i in x])\n",
    "\n",
    "binaries = df_bot_cp[df_bot_cp['type'] == 'binary']\n",
    "\n",
    "numerics = df_bot_cp[df_bot_cp['type'] == 'numeric']\n",
    "\n",
    "keep_cols = ['bot_question_id', 'question_title', 'title', 'cp_reveal_time', 'type', 'cp_question_id', 'cp_post_id', 'resolution', 'forecast_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and store problematic index\n",
    "problematic_idx = None\n",
    "for idx, row in groups.iterrows():\n",
    "   if len(row['forecast_values']) != len(row['options']):\n",
    "       problematic_idx = idx\n",
    "       break\n",
    "\n",
    "# Fix the specific row using stored index\n",
    "if problematic_idx is not None:\n",
    "   groups.at[problematic_idx, 'options'] = [\n",
    "       'Low',\n",
    "       'Moderate (or medium or equivalent)',\n",
    "       'High (or above such as Very High)'\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_exploded = groups.explode(['options', 'forecast_values'])\n",
    "groups_exploded['options'] = groups_exploded['options'].str.strip('\"')\n",
    "\n",
    "mask = groups_exploded['question_title'].str.contains('Will Joe Biden sign', case=False, na=False)\n",
    "groups_exploded.loc[mask, 'threshold'] = groups_exploded.loc[mask, 'question_title'].str.extract(r'(\\d+)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each bot_question_id\n",
    "for bot_question_id in groups_exploded['bot_question_id'].unique():\n",
    "    # Get all rows for this bot_question_id\n",
    "    question_group = groups_exploded[groups_exploded['bot_question_id'] == bot_question_id]\n",
    "    \n",
    "    # Get the question title\n",
    "    question_title = question_group['question_title'].iloc[0]\n",
    "    \n",
    "    # Function to check if option matches question title\n",
    "    def option_matches(row):\n",
    "        option = row['options']\n",
    "        if option in question_title:\n",
    "            return True\n",
    "        # Handle \"X or Y\" vs \"X-Y\" format\n",
    "        if '-' in option:\n",
    "            start, end = option.split('-')\n",
    "            or_format = f\"{start} or {end}\"\n",
    "            return or_format in question_title\n",
    "        return False\n",
    "    \n",
    "    # Find rows where the question title contains the option (with format handling)\n",
    "    matching_rows = question_group[question_group.apply(option_matches, axis=1)]\n",
    "    \n",
    "    filtered_rows = []\n",
    "\n",
    "    # If we found a matching row, add the first one to our filtered rows, EXCEPT... Biden\n",
    "    if not matching_rows.empty and 'Biden' not in question_title:\n",
    "        filtered_rows.append(matching_rows.iloc[0])\n",
    "    \n",
    "    # If Biden in question_title, we mustn't just take the first row - we must sum the rows that meet the threshold\n",
    "    if 'Biden' in question_title:\n",
    "        # Get first row for each unique option to avoid duplicates\n",
    "        first_rows = matching_rows.drop_duplicates(subset=['options'])\n",
    "\n",
    "        # Drop option='1' - we don't ask about 1 or more\n",
    "        first_rows = first_rows[first_rows['options'] != '1']\n",
    "        biden_interp = first_rows.copy()\n",
    "        \n",
    "        # Now for each row in biden_interp\n",
    "        for idx, row in biden_interp.iterrows():\n",
    "            threshold = int(row['threshold'])\n",
    "            # Calculate cumulative probability based on that row's threshold\n",
    "            if threshold == 2:\n",
    "                forecast_value = first_rows[first_rows['options'].isin(['2', '3', '4 or more'])]['forecast_values'].sum()\n",
    "            elif threshold == 3:\n",
    "                forecast_value = first_rows[first_rows['options'].isin(['3', '4 or more'])]['forecast_values'].sum()\n",
    "            elif threshold == 4:\n",
    "                forecast_value = first_rows[first_rows['options'] == '4 or more']['forecast_values'].sum()\n",
    "            \n",
    "            # Update this row's forecast value\n",
    "            biden_interp.at[idx, 'forecast_value'] = forecast_value\n",
    "        \n",
    "        filtered_rows.append(biden_interp.iloc[0])\n",
    "\n",
    "# Combine all filtered rows into a DataFrame\n",
    "groups_filtered = pd.DataFrame(filtered_rows)\n",
    "\n",
    "# Print check\n",
    "print(f\"Original unique multiple-choice bot_question_ids: {len(groups_exploded['bot_question_id'].unique())}\")\n",
    "print(f\"Filtered unique multiple-choice bot_question_ids: {len(groups_filtered['bot_question_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show me Biden rows in groups_exploded\n",
    "groups_filtered[groups_filtered['title'].str.contains('Biden', case=False)][['question_title', 'title', 'options', 'forecast_values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_filtered[['bot_question_id', 'question_title', 'title', 'options']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BINARIES: Interpret forecast_values as lists and take the 'yes' element from each\n",
    "binaries['forecast_values'] = binaries['forecast_values'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERICS ARE TRICKY\n",
    "\n",
    "# How long is each list in forecast_values?\n",
    "numerics['cdf_size'] = numerics['forecast_values'].apply(len)\n",
    "\n",
    "# Need to extract thresholds from binary versions of the numeric questions. TK: Could use another pair of eyes\n",
    "thresholds = {\n",
    "   29163: ('less', 2.0),        # COVID hospitalizations\n",
    "   29349: ('greater', 100),     # Brasilia rain\n",
    "   29350: ('greater', 150),     # Brasilia rain \n",
    "   29351: ('greater', 200),     # Brasilia rain\n",
    "   29353: ('greater', 20),      # Arms sales\n",
    "   29354: ('greater', 25),      # Arms sales\n",
    "   29362: ('greater', 3900),    # Emojis\n",
    "   29461: ('greater', 2000),    # Influenza hospitalizations\n",
    "   29462: ('greater', 2000),    # Influenza hospitalizations\n",
    "   29463: ('greater', 80),      # CDC influenza A\n",
    "   29566: ('less', 17.0),       # China unemployment Oct\n",
    "   29567: ('complicated', 0.0), # China unemployment Oct\n",
    "   29568: ('complicated', 0.0), # China unemployment Oct\n",
    "   29569: ('greater', 19.0),    # China unemployment Oct\n",
    "   29642: ('less', 0),        # Elon Musk net worth (less than or equal Bezos)\n",
    "   29643: ('complicated', 0.0), # Elon Musk net worth\n",
    "   29644: ('complicated', 0.0), # Elon Musk net worth\n",
    "   29645: ('complicated', 0.0), # Elon Musk net worth\n",
    "   29646: ('greater', 100),     # Elon Musk net worth (100+ more than Bezos)\n",
    "   29836: ('less', 17.0),       # China unemployment Nov\n",
    "   29837: ('complicated', 0.0), # China unemployment Nov\n",
    "   29838: ('complicated', 0.0), # China unemployment Nov\n",
    "   29839: ('greater', 19.0),    # China unemployment Nov\n",
    "   29836: ('greater', 375),     # NZ whooping cough\n",
    "   30578: ('complicated', 0.0), # NZ whooping cough\n",
    "   30579: ('less', 275),        # NZ whooping cough\n",
    "   30440: ('greater', -4),      # Trump favorability\n",
    "   30441: ('complicated', 0.0), # Trump favorability\n",
    "   30442: ('less', -6),         # Trump favorability\n",
    "   30583: ('greater', 7400),    # CAC 40\n",
    "   30584: ('complicated', 0.0), # CAC 40\n",
    "   30585: ('less', 7200),       # CAC 40\n",
    "   29462: ('complicated', 2000),    # Influenza hospitalizations\n",
    "   29462: ('complicated', 2000),     # Influenza hospitalizations\n",
    "   30791: ('greater', 19),      # Airline passengers\n",
    "   30792: ('complicated', 0.0),      # Airline passengers\n",
    "   30793: ('complicated', 0.0),         # Airline passengers\n",
    "   30794: ('less', 17),         # Airline passengers\n",
    "}\n",
    "\n",
    "# Apply that dictionary and make a 'binary_version_tuple' column\n",
    "numerics['binary_version_tuple'] = numerics['bot_question_id'].map(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values of binary_version_tuple\n",
    "unique_tuples = numerics['binary_version_tuple'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cdf's for the complicated ones (we will overwrite forecast_values)\n",
    "numerics['cdf'] = numerics['forecast_values']\n",
    "\n",
    "numerics = process_forecast_values(numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the forecast_values for the influenza hospitalizations questions (grouped by week)\n",
    "numerics[numerics['bot_question_id'].isin([29461, 29462])]['cdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doing the \"between\" (\"complicated\") numerics one by one by bot_question_id\n",
    "\n",
    "# 29503: Waymo exactly 4, i.e. between 3.5 and 4.5 on continuous question\n",
    "row = numerics[numerics['bot_question_id'] == 29503].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 3.5, 4.5)\n",
    "\n",
    "# 29567: China youth unemployment > 17.0 and less than 18.0\n",
    "row = numerics[numerics['bot_question_id'] == 29567].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 17.0, 18.0)\n",
    " \n",
    "# 29568: China youth unemployment > 18.0 and less than 19.0\n",
    "row = numerics[numerics['bot_question_id'] == 29568].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 18.0, 19.0)\n",
    "\n",
    "# 29643: Elon Musk net worth > 240 and less than 280\n",
    "row = numerics[numerics['bot_question_id'] == 29643].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 0, 40)\n",
    "\n",
    "# 29644: Elon Musk net worth > 280 and less than 310\n",
    "row = numerics[numerics['bot_question_id'] == 29644].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 40, 70)\n",
    "\n",
    "# 29645: Elon Musk net worth > 310 and less than 340\n",
    "row = numerics[numerics['bot_question_id'] == 29645].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 70, 100)\n",
    "\n",
    "# 29837: China youth unemployment > 17.0 and less than 18.0\n",
    "row = numerics[numerics['bot_question_id'] == 29837].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 17.0, 18.0)\n",
    "\n",
    "# 29838: China youth unemployment > 18.0 and less than 19.0\n",
    "row = numerics[numerics['bot_question_id'] == 29838].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values']= cdf_between(row, row['cdf'], 18.0, 19.0)\n",
    "\n",
    "# 30281: Waymo exactly 4, i.e. between 3.5 and 4.5 on continuous question\n",
    "row = numerics[numerics['bot_question_id'] == 30281].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 3.5, 4.5)\n",
    "\n",
    "# 30437: New Zealand >375 whooping cough cases\n",
    "row = numerics[numerics['bot_question_id'] == 30437].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 375, 400)\n",
    "\n",
    "# 30438: New Zealand >275 and less than 375 whooping cough cases\n",
    "row = numerics[numerics['bot_question_id'] == 30438].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 275, 375)\n",
    "\n",
    "# 30439: New Zealand less than 275 whooping cough cases\n",
    "row = numerics[numerics['bot_question_id'] == 30439].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 250, 275)\n",
    "\n",
    "# 30441: Trump net favorabilty > -6 and less than -4\n",
    "row = numerics[numerics['bot_question_id'] == 30441].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], -6, -4)\n",
    "\n",
    "# 30584: CAC 40 > 7200 and less than 7400\n",
    "row = numerics[numerics['bot_question_id'] == 30584].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 7200, 7400)\n",
    "\n",
    "# 30792: Airline passengers > 18 and less than 19\n",
    "row = numerics[numerics['bot_question_id'] == 30792].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 18, 19)\n",
    "\n",
    "# 30793: Airline passengers > 17 and less than 18\n",
    "row = numerics[numerics['bot_question_id'] == 30793].iloc[0]\n",
    "numerics.loc[numerics['bot_question_id'] == row['bot_question_id'], 'forecast_values'] = cdf_between(row, row['cdf'], 17, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = numerics[keep_cols]\n",
    "binaries = binaries[keep_cols]\n",
    "groups_filtered = groups_filtered[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can merge all back together into.... df_bot_cp_exploded; keep only the relevant columns, i.e. 'bot_question_id', 'cp_question_id', 'cp_post_id', 'resolution', 'forecast_values'\n",
    "df_bot_cp_exploded = pd.concat([groups_filtered, binaries, numerics])\n",
    "print(f'Number of rows: {len(df_bot_cp_exploded)}')\n",
    "df_bot_cp_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years from question_title and title\n",
    "df_bot_cp_exploded['bot_version_year'] = df_bot_cp_exploded['question_title'].apply(extract_year)\n",
    "df_bot_cp_exploded['cp_version_year'] = df_bot_cp_exploded['title'].apply(extract_year)\n",
    "\n",
    "cur_len = len(df_bot_cp_exploded)\n",
    "\n",
    "# Filter rows where the years do not match\n",
    "df_bot_cp_exploded = df_bot_cp_exploded[df_bot_cp_exploded['bot_version_year'] == df_bot_cp_exploded['cp_version_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract months from question_title and title\n",
    "df_bot_cp_exploded['bot_version_month'] = df_bot_cp_exploded['question_title'].apply(extract_month)\n",
    "df_bot_cp_exploded['cp_version_month'] = df_bot_cp_exploded['title'].apply(extract_month)\n",
    "\n",
    "if True:\n",
    "  # Filter rows where the months do not match\n",
    "  df_bot_cp_exploded = df_bot_cp_exploded[\n",
    "    (df_bot_cp_exploded['bot_version_month'] == df_bot_cp_exploded['cp_version_month']) | \n",
    "    (df_bot_cp_exploded['bot_version_month'].isnull())\n",
    "]\n",
    "\n",
    "  # How many rows were dropped?\n",
    "  print(f\"Number of rows dropped: {cur_len - len(df_bot_cp_exploded)}\")\n",
    "  print(f\"Remaining rows: {len(df_bot_cp_exploded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stragglers by hand\n",
    "bad_matches = [\n",
    "  30161, # Joe Biden no longer be president - CP version is \"before Jan 20\"\n",
    "  30723, # Doug Ford - CP version ends at the end of 2025\n",
    "  29463 # CDC flu - CP version asks about whole period thru April 2025\n",
    "]\n",
    "\n",
    "if 29356 in df_bot_cp_exploded['bot_question_id'].values:\n",
    "  df_bot_cp_exploded = df_bot_cp_exploded[~df_bot_cp_exploded['bot_question_id'].isin(bad_matches)]\n",
    "\n",
    "# And drop month and year columns out\n",
    "df_bot_cp_exploded = df_bot_cp_exploded[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to compute cp_baseline_score\n",
    "df_bot_cp_exploded['cp_baseline_score'] = df_bot_cp_exploded['forecast_values'].apply(compute_cp_baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_forecasts.merge(df_bot_cp_exploded[['bot_question_id', 'cp_post_id', 'cp_question_id', 'cp_reveal_time', 'forecast_values', 'cp_baseline_score' ]], on='bot_question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many forecast values are NA\n",
    "print(f\"Number of NA forecast values: {df_bot_cp_exploded['forecast_values'].isna().sum()}\")\n",
    "# Number of rows\n",
    "print(f\"Number of rows: {len(df_bot_cp_exploded)}\")\n",
    "# Number of each type based on type column\n",
    "print(df_bot_cp_exploded['type'].value_counts())\n",
    "\n",
    "# Show me the rows where forecast_values is NaN or 0\n",
    "df_bot_cp_exploded[df_bot_cp_exploded['forecast_values'].isna() | (df_bot_cp_exploded['forecast_values'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN in forecast_values\n",
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_cp_forecasts.dropna(subset=['forecast_values'])\n",
    "# Cast forecast_values as float\n",
    "df_top_bot_pro_cp_forecasts['forecast_values'] = df_top_bot_pro_cp_forecasts['forecast_values'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here begins the actual repeating of the bot-vs-pro analysis with bot-vs-CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique question ids? how many rows?\n",
    "print(f\"Number of unique question ids: {df_top_bot_pro_cp_forecasts['bot_question_id'].nunique()}\")\n",
    "print(f\"Number of rows: {len(df_top_bot_pro_cp_forecasts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "\n",
    "# Plot calibration curves for bot_team_median and pro_median\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'bot_team_median', 'Bot Team Median', 'blue')\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'pro_median', 'Pro Median', 'red')\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'forecast_values', 'Community Prediction', 'green')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Assigned Probability', fontsize=12)\n",
    "plt.ylabel('Fraction that Resolved \\'Yes\\'', fontsize=12)\n",
    "plt.title(f'Calibration Curve: Bot Team Median vs Pro Median vs Community Prediction\\n\\\n",
    "          (only overlap: {len(df_top_bot_pro_cp_forecasts)} questions)', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add diagonal line for perfect calibration\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "\n",
    "# Plot calibration curves for bot_team_median and pro_median\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts_all, 'bot_team_median', 'Bot Team Median', 'blue')\n",
    "plot_calibration_curve(df_top_bot_pro_forecasts, 'pro_median', 'Pro Median', 'red')\n",
    "plot_calibration_curve(df_top_bot_pro_cp_forecasts, 'forecast_values', 'Community Prediction', 'green')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Assigned Probability', fontsize=12)\n",
    "plt.ylabel('Fraction that Resolved \\'Yes\\'', fontsize=12)\n",
    "plt.title(f'Calibration Curve: Bot Team Median vs Pro Median vs Community Prediction\\n\\\n",
    "          all questions', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add diagonal line for perfect calibration\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make df_cp_baseline_wide and df_pro_bot_cp_resolved_questions\n",
    "df_cp_baseline_wide = df_top_bot_pro_cp_forecasts[['cp_post_id', 'bot_question_id', 'cp_baseline_score', 'forecast_values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create df_cp_bot_baseline_leaderboard, df_cp_bot_baseline_weighted_leaderboard\n",
    "\n",
    "# df_pro_bot_baseline_weights already has all the weights\n",
    "df_pro_bot_baseline_weights = df_pro_bot_baseline_weights.merge(df_cp_baseline_wide, on='bot_question_id', how='left')\n",
    "\n",
    "# Remove rows where cp_post_id is NaN (only want overlapping questions here)\n",
    "df_pro_bot_baseline_weights = df_pro_bot_baseline_weights.dropna(subset=['cp_post_id'])\n",
    "\n",
    "# Create a list of columns to keep\n",
    "forecaster_cols = ['cp_baseline_score', 'pro_median'] + [col for col in df_pro_bot_baseline_weights.columns if col in all_bots]\n",
    "df_filtered = df_pro_bot_baseline_weights[forecaster_cols]\n",
    "\n",
    "# Calculate the sum for each forecaster\n",
    "forecaster_scores = df_filtered.sum()\n",
    "forecaster_weighted_scores = df_filtered.mul(df_pro_bot_baseline_weights['question_weight'], axis=0).sum()\n",
    "\n",
    "question_counts = df_filtered.notna().sum()\n",
    "question_weighted_counts = df_filtered.notna().mul(df_pro_bot_baseline_weights['question_weight'], axis=0).sum()\n",
    "\n",
    "# Create a DataFrame for the leaderboard\n",
    "leaderboard = pd.DataFrame({\n",
    "    'Forecaster': forecaster_scores.index,\n",
    "    'Baseline': forecaster_scores.values,\n",
    "    'Count': question_counts.values\n",
    "})\n",
    "\n",
    "# Create a DataFrame for the leaderboard\n",
    "weighted_leaderboard = pd.DataFrame({\n",
    "    'Forecaster': forecaster_weighted_scores.index,\n",
    "    'Weighted_Baseline': forecaster_weighted_scores.values,\n",
    "    'Count': question_counts.values,\n",
    "    'Weighted Count': question_weighted_counts.values\n",
    "})\n",
    "\n",
    "# Sort the leaderboard by score in descending order\n",
    "leaderboard = leaderboard.sort_values('Baseline', ascending=False).reset_index(drop=True)\n",
    "weighted_leaderboard = weighted_leaderboard.sort_values('Weighted_Baseline', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add a 'Rank' column\n",
    "leaderboard['Rank'] = leaderboard.index + 1\n",
    "weighted_leaderboard['Rank'] = weighted_leaderboard.index + 1\n",
    "\n",
    "# Reorder columns to have Rank first\n",
    "leaderboard = leaderboard[['Rank', 'Forecaster', 'Baseline', 'Count']]\n",
    "weighted_leaderboard = weighted_leaderboard[['Rank', 'Forecaster', 'Weighted_Baseline', 'Count', 'Weighted Count']]\n",
    "\n",
    "# Round to one decimal place\n",
    "leaderboard['Baseline'] = leaderboard['Baseline'].round(1)\n",
    "weighted_leaderboard['Weighted_Baseline'] = weighted_leaderboard['Weighted_Baseline'].round(1)\n",
    "weighted_leaderboard['Weighted Count'] = weighted_leaderboard['Weighted Count'].round(1)\n",
    "\n",
    "#leaderboard\n",
    "weighted_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create df_cp_bot_forecasts, df_bot_vs_cp_peer\n",
    "\n",
    "df_cp_forecasts = df_cp_baseline_wide[['cp_post_id', 'bot_question_id', 'forecast_values']]\n",
    "\n",
    "want_cols = ['bot_question_id'] + [col for col in df_pro_bot_forecasts.columns if col in all_bots]\n",
    "\n",
    "df_cp_bot_forecasts = df_cp_forecasts.merge(df_pro_bot_forecasts[want_cols], on='bot_question_id', how='left')\n",
    "\n",
    "df_cp_bot_forecasts = df_cp_bot_forecasts.merge(df_top_bot_pro_forecasts[['bot_question_id', 'resolution', 'question_weight']], on='bot_question_id', how='left')\n",
    "\n",
    "# Create a new DataFrame to store peer scores\n",
    "df_bot_vs_cp_peer = df_cp_bot_forecasts.copy()\n",
    "df_bot_vs_cp_peer = df_bot_vs_cp_peer[['resolution', 'question_weight', 'bot_question_id']]\n",
    "\n",
    "# Calculate peer score for each bot\n",
    "for bot in all_bots:\n",
    "    # Calculate Head-to-head score based on the condition\n",
    "    peer_score = np.where(\n",
    "        df_cp_bot_forecasts['resolution'] == 'yes',\n",
    "        np.log(df_cp_bot_forecasts[bot] / df_cp_bot_forecasts['forecast_values']),\n",
    "        np.log((1 - df_cp_bot_forecasts[bot]) / (1 - df_cp_bot_forecasts['forecast_values']))\n",
    "    )\n",
    "\n",
    "    # Add the calculated peer score to the new DataFrame\n",
    "    df_bot_vs_cp_peer[bot] = 100 * peer_score\n",
    "\n",
    "# Calculate Head-to-head score for bot_team (TK: bot TEAM or median)\n",
    "peer_score = np.where(\n",
    "    df_cp_bot_forecasts['resolution'] == 'yes',\n",
    "    np.log(df_cp_bot_forecasts['bot_median'] / df_cp_bot_forecasts['forecast_values']),\n",
    "    np.log((1 - df_cp_bot_forecasts['bot_median']) / (1 - df_cp_bot_forecasts['forecast_values']))\n",
    ")\n",
    "\n",
    "# Add the calculated peer score to the new DataFrame\n",
    "df_bot_vs_cp_peer[\"bot_median\"] = 100 * peer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Weighted head-to-head, T test\n",
    "\n",
    "\"\"\"\n",
    "df_W_leaderboard: A leaderboard based on df_bot_vs_cp_peer with question\n",
    "weighting and the calculations for doing a weighted T test\n",
    "\"\"\"\n",
    "\n",
    "forecaster_weighted_scores = forecaster_weighted_scores.fillna(0)\n",
    "\n",
    "# Cast weights as numeric\n",
    "df_bot_vs_cp_peer['question_weight'] = pd.to_numeric(df_bot_vs_cp_peer['question_weight'], errors='coerce')\n",
    "\n",
    "df_W_leaderboard = calculate_t_test(df_bot_vs_cp_peer, all_bots)\n",
    "\n",
    "df_W_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write both leaderboards to csv\n",
    "weighted_leaderboard.to_csv('weighted_baseline_bot_cp.csv', index=False)\n",
    "\n",
    "df_W_leaderboard.to_csv('weighted_t_test_h2h_bot_vs_cp.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique question ids in df_top_bot_pro_cp_forecasts\n",
    "print(f\"Number of unique question ids: {len(df_top_bot_pro_cp_forecasts['bot_question_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS JUST ON THE 43 THAT THEY ALL FORECASTED ON\n",
    "\n",
    "# Recommend paying attention to the bot team h2h scores vs CP graph (further down) rather than pgodzinai (he was selected as the bot \"team\" vs the PROS)\n",
    "\n",
    "df_top_bot_pro_cp_forecasts['head_to_head_bot_vs_cp'] = df_top_bot_pro_cp_forecasts.apply(calculate_head_to_head, args=('bot_team_median', 'forecast_values'), axis=1)\n",
    "df_top_bot_pro_cp_forecasts['head_to_head_cp_vs_pro'] = df_top_bot_pro_cp_forecasts.apply(calculate_head_to_head, args=('forecast_values', 'pro_median'), axis=1)\n",
    "df_top_bot_pro_cp_forecasts['head_to_head_bot_vs_pro'] = df_top_bot_pro_cp_forecasts.apply(calculate_head_to_head, args=('bot_team_median', 'pro_median'), axis=1)\n",
    "\n",
    "plot_head_to_head_distribution(df_top_bot_pro_cp_forecasts, 'head_to_head_bot_vs_cp', ('pgodzinai', 'CP'))\n",
    "plot_head_to_head_distribution(df_top_bot_pro_cp_forecasts, 'head_to_head_cp_vs_pro', ('CP', 'Pro median'))\n",
    "plot_head_to_head_distribution(df_top_bot_pro_cp_forecasts, 'head_to_head_bot_vs_pro', ('pgodzinai', 'Pro median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Weighted Bot Only Peer, T test (FOR CP COMPARISON)\n",
    "\n",
    "# To choose our top bot team, we only use the questions for which there is no CP benchmark\n",
    "yes_cp_benchmark = df_top_bot_pro_cp_forecasts[~df_top_bot_pro_cp_forecasts['cp_post_id'].isna()]['bot_question_id'].values\n",
    "\n",
    "df_bot_only_peer = df_bot_peer[~df_bot_peer['bot_question_id'].isin(yes_cp_benchmark)]\n",
    "df_bot_only_peer_wide = make_wide(df_bot_only_peer, df_pro_bot_resolved_questions)\n",
    "\n",
    "df_W_bot_only_peer_leaderboard = calculate_t_test(df_bot_only_peer_wide, df_bot_only_peer['forecaster'].unique())\n",
    "\n",
    "#df_W_bot_only_peer_leaderboard[['W_ave', 'W_count', 'lower_bound', 'upper_bound']].sort_values(by='lower_bound', ascending=False)\n",
    "\n",
    "# Sort the DataFrame by the lower_bound column in descending order\n",
    "sorted_df = df_W_bot_only_peer_leaderboard.sort_values(by='lower_bound', ascending=False)\n",
    "\n",
    "# exclude bot median for purposes of bot teaming\n",
    "sorted_df = sorted_df.drop('bot_median', errors='ignore')\n",
    "\n",
    "# Get the top 10 bot names\n",
    "top_10_bots = sorted_df.index[:10].tolist()\n",
    "\n",
    "# Print the list of top 10 bots\n",
    "print(\"Top 10 bots:\")\n",
    "for i, bot in enumerate(top_10_bots, 1):\n",
    "    print(f\"{i}. {bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Calculate df_bot_team_forecasts\n",
    "\n",
    "df_bot_team_forecasts = pd.merge(\n",
    "    df_bot_forecasts,\n",
    "    df_pro_bot_resolved_questions[['bot_question_id', 'pro_question_id', 'question_weight', 'resolution']],\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Keep only rows where the there is no CP benchmark\n",
    "df_bot_team_forecasts = df_bot_team_forecasts[~df_bot_team_forecasts['bot_question_id'].isin(yes_cp_benchmark)]\n",
    "\n",
    "columns_to_keep = ['bot_question_id', 'question_weight', 'resolution'] + top_10_bots\n",
    "\n",
    "# Filter the DataFrame to keep only the specified columns\n",
    "df_bot_team_forecasts = df_bot_team_forecasts[columns_to_keep]\n",
    "\n",
    "# Function to calculate median forecast for a given number of bots\n",
    "def calculate_median_forecast(df, bots):\n",
    "    return df[bots].median(axis=1)\n",
    "\n",
    "# Calculate and add median forecasts for 2 to 10 bots\n",
    "for i in range(1, 11):\n",
    "    bots_subset = top_10_bots[:i]\n",
    "    column_name = f'median_forecast_{i}_bots'\n",
    "    df_bot_team_forecasts[column_name] = calculate_median_forecast(df_bot_team_forecasts, bots_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Calculate the baseline scores for each team size\n",
    "\n",
    "teams = ['median_forecast_1_bots',\n",
    "         'median_forecast_2_bots',\n",
    "         'median_forecast_3_bots',\n",
    "         'median_forecast_4_bots',\n",
    "         'median_forecast_5_bots',\n",
    "         'median_forecast_6_bots',\n",
    "         'median_forecast_7_bots',\n",
    "         'median_forecast_8_bots',\n",
    "         'median_forecast_9_bots',\n",
    "         'median_forecast_10_bots']\n",
    "\n",
    "weighted_scores = calculate_weighted_scores(df_bot_team_forecasts, teams)\n",
    "\n",
    "# Print nicely - round to 2 decimal places and first column should be just an integer (bot team size)\n",
    "weighted_scores_print = pd.DataFrame(weighted_scores).reset_index()\n",
    "weighted_scores_print.columns = ['Bot_Team_Size', 'Weighted_Baseline_Score_for_Bot_Team_Median']\n",
    "weighted_scores_print['Weighted_Baseline_Score_for_Bot_Team_Median'] = weighted_scores_print['Weighted_Baseline_Score_for_Bot_Team_Median'].round(2)\n",
    "weighted_scores_print['Bot_Team_Size'] = weighted_scores_print['Bot_Team_Size'].apply(lambda x: int(x.split('_')[2].split('_')[0]))\n",
    "weighted_scores_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of top bot team from weighted_scores_print?\n",
    "winning_bot_team_size = weighted_scores_print.sort_values(by='Weighted_Baseline_Score_for_Bot_Team_Median', ascending=False).head(1)['Bot_Team_Size'].values[0]\n",
    "top_bot_team = top_10_bots[:winning_bot_team_size]\n",
    "top_bot_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Weighted team-vs-cp\n",
    "\n",
    "# We have our top bot team members.\n",
    "# Create df with bot_question_id, forecasts, resolution, weights\n",
    "# Calculate the head-to-head scores\n",
    "\n",
    "df_top_bot_forecasts = df_bot_forecasts[['bot_question_id'] + top_bot_team]\n",
    "df_top_bot_forecasts['bot_team_median'] = df_top_bot_forecasts[top_bot_team].median(axis=1)\n",
    "\n",
    "df_cp = df_top_bot_pro_cp_forecasts[['cp_post_id', 'bot_question_id', 'forecast_values', 'resolution', 'question_weight']]\n",
    "\n",
    "df_top_bot_cp_forecasts = pd.merge(\n",
    "    df_top_bot_forecasts,\n",
    "    df_cp,\n",
    "    on='bot_question_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter to only those rows where CP is not NA\n",
    "df_top_bot_cp_forecasts = df_top_bot_cp_forecasts.dropna(subset=['forecast_values'])\n",
    "\n",
    "# Add the head_to_head column\n",
    "df_top_bot_cp_forecasts['head_to_head'] = df_top_bot_cp_forecasts.apply(calculate_head_to_head, args=('bot_team_median', 'forecast_values'), axis=1)\n",
    "\n",
    "display_head_and_tail(df_top_bot_cp_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge this bot_team_median with df_top_bot_pro_cp_forecasts. First, rename bot_team_median in df_top_bot_pro_cp_forecasts to pgodzinai\n",
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_cp_forecasts.rename(columns={'bot_team_median': 'pgodzinai'})\n",
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_cp_forecasts.merge(df_top_bot_cp_forecasts[['bot_question_id', 'bot_team_median']], on='bot_question_id', how='left')\n",
    "df_top_bot_pro_cp_forecasts = df_top_bot_pro_cp_forecasts.rename(columns={'forecast_values': 'community_prediction'})\n",
    "\n",
    "# Write df_top_bot_pro_cp_forecasts to csv, but only the columns bot question id, cp post id, cp question id, title, resolution, cp_reveal_time, forecast_values, bot_team_median, pro_median\n",
    "df_top_bot_pro_cp_forecasts[['bot_question_id', 'cp_post_id', 'cp_question_id', 'title', 'resolution', 'cp_reveal_time', 'community_prediction', 'bot_team_median', 'pgodzinai', 'pro_median']].to_csv('df_top_bot_pro_cp_forecasts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_total_score = get_weighted_score(df_top_bot_cp_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot head-to-head distribution\n",
    "plot_head_to_head_distribution(df_top_bot_cp_forecasts, 'head_to_head', ('Bot Team (pgodzinai, MWG, annabot)', 'CP'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
